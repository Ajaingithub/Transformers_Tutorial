{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ce8dd5",
   "metadata": {},
   "source": [
    "## Fine Tuning a Llama2 as a chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b207a",
   "metadata": {},
   "source": [
    "<p>After pretraining learning make more series from Andrej Karpathy. </p>\n",
    "<p>Learning how we can do Fine Tuning using this video from Krish Naik https://www.youtube.com/watch?v=iOdFUJiB0Zc</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.49.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.0\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall bitsandbytes -y\n",
    "# !pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (25.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (78.1.1)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.45.1)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools, pip\n",
      "\u001b[2K  Attempting uninstall: setuptools\n",
      "\u001b[2K    Found existing installation: setuptools 78.1.1\n",
      "\u001b[2K    Uninstalling setuptools-78.1.1:\n",
      "\u001b[2K      Successfully uninstalled setuptools-78.1.1\u001b[0m \u001b[32m0/2\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pip━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: pip 25.2━━━\u001b[0m \u001b[32m0/2\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling pip-25.2:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [pip]\n",
      "\u001b[2K      Successfully uninstalled pip-25.2[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [pip]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pip]\u001b[32m1/2\u001b[0m [pip]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pip-25.3 setuptools-80.9.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.41.2\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface_hub==0.23.4\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface_hub==0.23.4) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface_hub==0.23.4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2025.11.12)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Installing collected packages: huggingface_hub, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.36.0\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.36.0:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.36.0\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Found existing installation: transformers 4.41.12m0/2\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling transformers-4.41.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.41.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.23.4 which is incompatible.\n",
      "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.23.4 transformers-4.41.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.41.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.41.2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (4.41.2)\n",
      "Collecting huggingface-hub==0.26.5\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers==4.41.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub==0.26.5) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub==0.26.5) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2025.11.12)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.2.3\n",
      "    Uninstalling huggingface_hub-1.2.3:\n",
      "      Successfully uninstalled huggingface_hub-1.2.3\n",
      "Successfully installed huggingface-hub-0.26.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \\\n",
    "  transformers==4.41.2 \\\n",
    "  huggingface-hub==0.26.5 \\\n",
    "  tokenizers==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.19.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from tokenizers==0.19.1) (0.26.5)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea5e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tokenizers\n",
    "# tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.19.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from tokenizers==0.19.1) (0.26.5)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.2)\n",
      "Requirement already satisfied: requests in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade tokenizers==0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65d86a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: peft in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.10.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: trl in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.7.11)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.26.5)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tokenizers in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (0.19.1)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: psutil in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from trl) (4.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: anyio in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.12.0)\n",
      "Requirement already satisfied: certifi in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: idna in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "Installing collected packages: huggingface_hub, tokenizers, transformers, peft, trl\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.26.5\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.26.5:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.26.5\n",
      "\u001b[2K  Attempting uninstall: tokenizers━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1[32m0/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:━━━━━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.1 \u001b[32m0/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [tokenizers]\n",
      "\u001b[2K    Found existing installation: transformers 4.41.2━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.41.2:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.41.2━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: peft\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: peft 0.10.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling peft-0.10.0:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled peft-0.10.0━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: trl━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]ers]\n",
      "\u001b[2K    Found existing installation: trl 0.7.11[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K    Uninstalling trl-0.7.11:\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K      Successfully uninstalled trl-0.7.11m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [peft]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [trl]\u001b[32m4/5\u001b[0m [trl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface_hub-0.36.0 peft-0.18.0 tokenizers-0.22.1 transformers-4.57.3 trl-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \\\n",
    "  transformers \\\n",
    "  peft \\\n",
    "  trl \\\n",
    "  huggingface_hub \\\n",
    "  tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b248d7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d6254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # To load the Causal lang. Model like ChatGPT\n",
    "    AutoTokenizer, # To load the tokenization\n",
    "    BitsAndBytesConfig, # for low memory convert to low precision like FP32 --> FP16\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments, # passing Training arguments\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from transformers import LlamaTokenizer\n",
    "from peft import LoraConfig, PeftModel # Parameter efficient Fine Tuning. LORA performs matrix decomposition which makes the matrix quite small. There is QLORA which works on Quantization LORA.\n",
    "from trl import SFTTrainer # Train transformer language models with reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba34417",
   "metadata": {},
   "source": [
    "- Orignal Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "- Reformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k\n",
    "- Complete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2\n",
    "\n",
    "#**How to fine tune Llama 2**\n",
    "- We also need to consider the overhead due to optimizer states, gradients, and forward activations\n",
    "- Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\n",
    "- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here.\n",
    "\n",
    "1. Load a llama-2-7b-chat-hf model (chat model)\n",
    "2. Train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model Llama-2-7b-chat-finetune\n",
    "\n",
    "QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0170f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "# Non-chat version → worse conversational behavior unless heavily fine-tuned.\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "# The instruction dataset to use\n",
    "# Small (~1k examples) → good for quick adaptation, not general knowledge learning.\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64 ## This is the rank by which matrix decomposition takes place. \n",
    "# suppose if lora_r = 1 then 3 X 3 matrix is decomposed to 3 X 1 and 1 X 3.\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "# Scaling factor.\n",
    "# Effective LoRA contribution ≈ alpha / r = 16 / 64 = 0.25\n",
    "lora_alpha = 16 # scaling factor\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Even though your model weights are stored in 4-bit, the actual matrix multiplications are NOT done in 4-bit.\n",
    "# Instead:\n",
    "# Weights are dequantized on the fly \n",
    "# computation happens in float16\n",
    "# 4-bit weights  →  dequantize →  float16 compute → output\n",
    "\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "# NF4 (NormalFloat4) is optimized for neural network weights.\n",
    "# Better accuracy than FP4. \n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Original fp16 model\n",
    "#         ↓\n",
    "# Quantized to NF4 (4-bit)   ← bnb_4bit_quant_type=\"nf4\"\n",
    "#         ↓\n",
    "# Stored efficiently in GPU memory\n",
    "#         ↓\n",
    "# Dequantized to fp16        ← bnb_4bit_compute_dtype=\"float16\"\n",
    "#         ↓\n",
    "# Matrix multiplication & gradients\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True # (half precision) 1 bit sign, 5 bits exponent, 10 bits mantissa (fraction)\n",
    "bf16 = False # 1 bit sign, 8 bits exponent, 7 bits mantissa\n",
    "# Same exponent range as fp32\n",
    "# Much more numerically stable than fp16\n",
    "# Lower precision than fp16\n",
    "# Large-scale model training (TPUs, modern GPUs like A100, H100)\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# (forward → backward) × N → optimizer.step() where N is the number of minibatches \n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Training transformers normally stores all intermediate activations so gradients can be computed during backprop.\n",
    "# For a 7B model, this eats huge GPU memory.\n",
    "# Gradient checkpointing:\n",
    "# Does NOT store all activations\n",
    "# Recomputes them during backward pass instead\n",
    "# Trades compute time for memory savings\n",
    "# so instead of Forward → store activations → Backward\n",
    "# It do\n",
    "# Forward → discard activations\n",
    "# Backward → recompute forward → compute gradients\n",
    "\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Weight decay is a regularization technique that prevents a model from overfitting by discouraging weights from becoming too large.\n",
    "# Think of it as telling the model:\n",
    "# “Fit the data well, but don’t rely on extremely large parameter values.”\n",
    "# During training, models can reduce loss by:\n",
    "# Making some weights very large\n",
    "# Memorizing training examples\n",
    "# Weight decay penalizes large weights, pushing them slightly toward zero at every update.\n",
    "# Without weight decay:\n",
    "# w = w - lr * gradient\n",
    "# With weight decay:\n",
    "# w = w - lr * (gradient + λ * w)\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "# Early training → need large steps to learn quickly\n",
    "# Later training → need small steps to fine-tune\n",
    "# Too large LR → divergence\n",
    "# Too small LR → slow or stuck training\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40d746",
   "metadata": {},
   "source": [
    "#### Since Llama 2 tokenizer does not work on the newer version. \n",
    "So we have saved the tokenizer on the older version and updated all the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede98fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56780d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"/mnt/data/projects/.immune/Personal/Transformers_Tutorial/Llama/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama2_tokenizer_frozen/tokenizer_config.json',\n",
       " './llama2_tokenizer_frozen/special_tokens_map.json',\n",
       " './llama2_tokenizer_frozen/tokenizer.model',\n",
       " './llama2_tokenizer_frozen/added_tokens.json',\n",
       " './llama2_tokenizer_frozen/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_dir = \"./llama2_tokenizer_frozen\"\n",
    "# tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54118e",
   "metadata": {},
   "source": [
    "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
    "\n",
    "2. Then, we’re configuring bitsandbytes for 4-bit quantization.\n",
    "\n",
    "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
    "\n",
    "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "dataset = load_dataset(dataset_name, split=\"train\") ## Dataset to load\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype) # bnb_4bit_compute_dtype = float 16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit, # Quantization is performed at 4 bits\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type, # nf4 that is more precised\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # float 16 caluclation is perform with fp16 for better accuracy\n",
    "    bnb_4bit_use_double_quant=use_nested_quant, # use_nested_quant = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0850718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49.0\n"
     ]
    }
   ],
   "source": [
    "# !python -c \"import bitsandbytes as bnb; print(bnb.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff261d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d46a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Your GPU does not supports bfloat16: accelerate training with bf16=True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    print(major)\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"Your GPU does not supports bfloat16: accelerate training with bf16=True\")\n",
    "# CUDA compute capability:\n",
    "# 7.x → Volta / Turing (V100, T4)\n",
    "# 8.0+ → Ampere (A100)\n",
    "# 9.0+ → Hopper (H100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9787f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NousResearch/Llama-2-7b-chat-hf BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": false,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      " {'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model_name, bnb_config, device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50d5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # Load the entire model on the GPU 0\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b4af29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839f906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"dtype\": \"float16\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d00ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we saved using old version, we just need to upload it on the newer version.\n",
    "from transformers import AutoTokenizer\n",
    "os.chdir(\"/mnt/data/projects/.immune/Personal/Transformers_Tutorial/Llama/\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"llama2_tokenizer_frozen\",\n",
    "    use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f906e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='llama2_tokenizer_frozen', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7cb94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e032dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results 1 4 1 paged_adamw_32bit 0 25 0.0002 0.001 True False 0.3 -1 0.03 True cosine\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "# https://huggingface.co/docs/transformers/v5.0.0rc1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "print(\n",
    "    output_dir, num_train_epochs, per_device_train_batch_size, gradient_accumulation_steps,\n",
    "    optim,save_steps,logging_steps,learning_rate,weight_decay,fp16,bf16,max_grad_norm,max_steps,\n",
    "    warmup_ratio,group_by_length,lr_scheduler_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d142dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "# https://huggingface.co/docs/transformers/v5.0.0rc1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d59393e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=True,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=no,\n",
       "eval_use_gather_object=False,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=True,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=no,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.0002,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results/runs/Dec25_06-43-33_ajaingcpgpu,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=25,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=cosine,\n",
       "max_grad_norm=0.3,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=paged_adamw_32bit,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "parallelism_config=None,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=4,\n",
       "prediction_loss_only=False,\n",
       "project=huggingface,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=None,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=0,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "trackio_space_id=trackio,\n",
       "use_cpu=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.03,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8536664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=64, target_modules=None, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(peft_config)\n",
    "print(packing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cad59645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>',\n",
       " '<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\\n\\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \\n\\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \\n\\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>',\n",
       " '<s>[INST] Compose a professional email with the following points:\\n\\nMe chinese cook 10 years\\nGood good cook\\nPeople love my food\\nCan you hire me? [/INST] Thanks for the information. Unfortunately, your initial message does not contain enough details or structure to compose a compelling professional email to a potential employer. Here are some suggestions for how we can improve the message:\\n\\n1) Let\\'s first clarify the type of cooking position you are interested in. Are you looking for a job as a line cook, sous chef, executive chef, or something else? This will help determine how to emphasize relevant experience and skills.\\n\\n2) What specific experience do you have? For example, \"Over 10 years of experience preparing authentic Sichuan and Cantonese cuisine.\" Highlight any relevant skills, like knife skills, cooking techniques, menu development, etc. Mention the types of cuisine you specialize in.\\n\\n3) Have you worked in a professional kitchen before? If so, mention the name and type of restaurant (upscale, casual dining, etc.). Emphasize relevant responsibilities and accomplishments. If not, that\\'s fine too, just be clear that you\\'re looking to transition into a professional kitchen and highlight related skills and passion for cooking.\\n\\n4) Do you have any certifications or training? Note any culinary degrees, certificates, or other credentials. Even informal training can show your dedication to the craft.\\n\\n5) Are you looking for full-time, part-time, temporary, or contract work? Be specific about the type of opportunity you\\'re seeking.\\n\\nWith the answers to these questions, we can craft an email in a standard business letter format. The opening can express your interest in the position and summarize your relevant experience and qualifications. The body can go into more depth on your experience, skills, and goals. The closing can reiterate your interest and mention that you\\'d welcome the opportunity to further discuss the role.\\n\\nDoes this help? Let me know if you would like help crafting the full email. I\\'m happy to assist you in composing a professional and compelling message to potential employers. </s>',\n",
       " '<s>[INST] ¿Qué juegos me recomendarías si me ha gustado Undertale? [/INST] Deltarune: Este es un juego creado por el mismo desarrollador de Undertale, Toby Fox. Al igual que Undertale, Deltarune combina elementos de RPG con una historia divertida y personajes memorables.\\n\\nEarthBound: Este es un juego clásico de RPG que se ha convertido en un cult classic en todo el mundo. Al igual que Undertale, EarthBound combina elementos de comedia y drama en una historia única y memorable.\\n\\nHollow Knight: Este es un juego de aventura y metroidvania que cuenta con un mundo oscuro y misterioso, así como personajes interesantes y una banda sonora memorable.\\n\\nHyper Light Drifter: Este es un juego de aventura y acción que cuenta con un mundo hermoso y misterioso, así como combates desafiantes y una banda sonora impactante.\\n\\nOri and the Blind Forest: Este es un juego de aventura y plataformas que cuenta con un mundo hermoso y una historia conmovedora.\\n\\nEstos son solo algunos ejemplos de juegos similares a Undertale. Cualquiera de estos juegos podría ser una buena opción para aquellos que disfrutaron de Undertale y buscan algo similar. </s><s>[INST] EarthBound suena divertido, puedes contarme más sobre su historia [/INST] \"EarthBound\" es un juego de rol clásico de la consola Super Nintendo Entertainment System (SNES) que fue desarrollado y publicado por Nintendo en 1995. La historia del juego sigue a un joven llamado Ness y a sus amigos en un viaje épico para salvar el mundo.\\n\\nEl juego comienza en la ciudad de Onett, donde Ness es despertado por un extraño sueño. Después de investigar, descubre que un asteroide ha caído cerca de su casa y decide investigar. Allí, Ness encuentra a un extraterrestre llamado Buzz Buzz que le da la misión de salvar el mundo de un ser maligno llamado Giygas.\\n\\nA lo largo del juego, Ness viaja por todo el mundo, luchando contra enemigos y haciendo amigos con otros personajes. Algunos de sus compañeros de aventura incluyen a Paula, una telepata, y Jeff, un inventor prodigio. Juntos, deben enfrentarse a una serie de desafíos y desafiantes jefes finales para cumplir su misión y salvar al mundo.\\n\\n\"EarthBound\" se destaca por su estilo único, que combina elementos de juegos de rol clásicos con una ambientación surreal y humorística. También es conocido por su banda sonora inolvidable y sus personajes carismáticos. Aunque el juego no fue un gran éxito en su lanzamiento, ha ganado un culto de seguidores a lo largo de los años y es considerado por muchos como un clásico de los juegos de rol. </s>',\n",
       " '<s>[INST] Cual es el desarrollo motor de niño/a de 0 a 6 años [/INST] El desarrollo motor de un niño/a de 0 a 6 años puede ser dividido en varios hitos importantes, que suceden en un orden predecible pero que pueden variar en el tiempo que tardan en alcanzarse. Algunos de estos hitos incluyen:\\n\\nDesde el nacimiento hasta los 3 meses: Los bebés tienen movimientos reflejos y pueden levantar la cabeza brevemente cuando están boca abajo. Comienzan a seguir objetos con la mirada y pueden empezar a sostener objetos con las manos.\\nDe 3 a 6 meses: Los bebés empiezan a rodar de lado a lado y a sentarse con apoyo. También pueden empezar a gatear y a hacer movimientos más coordinados con las manos.\\nDe 6 a 9 meses: Los bebés empiezan a sentarse sin apoyo y pueden comenzar a gatear hacia adelante. También pueden sostener objetos con la mano y pueden empezar a transferir objetos de una mano a otra.\\nDe 9 a 12 meses: Los bebés pueden empezar a caminar con apoyo y a levantarse sin ayuda. También pueden empezar a gatear hacia atrás y a gatear en posición cuadrúpeda. Además, pueden comenzar a utilizar la pincer grip (utilizando el dedo índice y el pulgar) para agarrar objetos.\\nDe 1 a 2 años: Los niños/as pueden caminar sin ayuda y empezar a correr y saltar. También pueden empezar a subir escaleras, utilizando ambos pies para subir un escalón a la vez. Además, pueden empezar a usar utensilios para comer y a realizar dibujos más precisos con un lápiz o crayón.\\nDe 2 a 3 años: Los niños/as pueden subir escaleras utilizando un pie por escalón. También pueden empezar a pedalear en una bicicleta y saltar en un pie. Además, pueden empezar a vestirse y desvestirse solos, y a construir torres y estructuras simples con bloques.\\nDe 3 a 6 años: Los niños/as pueden saltar y correr con mayor coordinación y equilibrio, y pueden comenzar a realizar actividades físicas más complejas como nadar y jugar a juegos deportivos. También pueden comenzar a escribir y dibujar letras y números, y a realizar tareas más complejas como cortar con tijeras y manipular pequeños objetos.\\nEs importante tener en cuenta que estos hitos son solo una guía general y que cada niño/a se desarrolla a su propio ritmo. Algunos niños/as pueden alcanzar estos hitos más temprano o más tarde que otros, y eso es normal. Si tienes alguna preocupación sobre el desarrollo motor de tu hijo/a, siempre es recomendable hablar con un médico o especialista en desarrollo infantil. </s><s>[INST] Y el desarrollo del habla? [/INST] El desarrollo del habla en un niño o niña ocurre de manera progresiva y gradual, y puede variar en cada niño/a. A continuación, se presenta un resumen del desarrollo del habla en diferentes etapas:\\n\\n- De 0 a 6 meses: El bebé emite sonidos y vocalizaciones, como gorjeos, risas, llantos y gritos. Alrededor de los 4 meses, comienza a balbucear y a producir sonidos con mayor variedad de consonantes y vocales, como \"bababa\" o \"papapa\".\\n- De 6 a 12 meses: El bebé comienza a imitar los sonidos que escucha, y puede intentar decir algunas palabras simples como \"mamá\" o \"papá\". También puede usar gestos, como señalar objetos o levantar los brazos para que lo levanten.\\n- De 1 a 2 años: El niño o niña comienza a combinar palabras para formar frases simples, como \"más leche\" o \"quiero juguete\". A esta edad, el habla aún puede ser difícil de entender para personas que no están familiarizadas con el niño/a.\\n- De 2 a 3 años: El niño/a amplía su vocabulario y su capacidad para formar oraciones más complejas. También comienza a hacer preguntas, a utilizar pronombres y a emplear el plural. A esta edad, el habla se vuelve más clara y comprensible para las personas que no conviven diariamente con el niño/a.\\n- De 3 a 4 años: El niño/a comienza a utilizar frases más complejas y a contar historias con cierta coherencia. También es capaz de seguir instrucciones más detalladas y de responder a preguntas que requieren más reflexión.\\n- De 4 a 6 años: El niño/a sigue ampliando su vocabulario y su capacidad para formular oraciones complejas. También comienza a leer y escribir, lo que contribuye a su desarrollo lingüístico en general. A esta edad, el habla es fluida y bien estructurada.\\n\\nEs importante tener en cuenta que el desarrollo del habla y el lenguaje puede variar entre los niños y niñas, y que algunos pueden requerir ayuda adicional en caso de presentar retrasos o dificultades en su desarrollo. En caso de preocupación, se recomienda consultar a un especialista en el área de la salud infantil. </s>',\n",
       " '<s>[INST] J\\'habite dans le centre de Paris et je ne sais pas où aller faire mes courses pour ne pas payer trop cher. Quel supermarché est considéré comme étant le moins cher ? [/INST] Les drives sont les supermarchés les moins chers. Voici une liste des drives de Paris du 2ème au 6ème arrondissement:\\n\\nParis 2ème arrondissement\\n\\tAuchan Drive – Paris Réaumur (2ème)\\n    Courses U Drive – Paris Petits Carreaux (2ème)\\n\\nParis 3ème arrondissement\\n\\tCourses U Drive – Paris Temple (3ème)\\n\\nParis 4ème arrondissement\\n\\tCarrefour Drive – Paris Beaubourg (4ème)\\n\\nParis 5ème arrondissement\\n\\tAuchan Drive – Paris Pierre Nicole (5ème)\\n\\tAuchan Drive – Piéton Paris Monge (5ème)\\n\\tAuchan Drive – Piéton Paris Saint Germain (5ème)\\n\\tAuchan Drive – Piéton Paris Claude Bernard (5ème)\\n\\tCarrefour Drive – Paris Pascal (5ème)\\n\\tCarrefour Drive – Paris Lacépède (5ème)\\n\\tDrive Intermarché – Paris Fossés Saint-Bernard (5ème)\\nParis 6ème arrondissement\\n\\tAuchan Drive – Piéton Paris Assas (6ème)\\n\\tAuchan Drive – Piéton Paris Bréa (6ème)\\n\\tCarrefour Drive – Paris Notre Dame des Champs (6ème) </s><s>[INST] Qu\\'est ce qu\\'un drive pour piéton ? Un drive = voiture ? [/INST] En effet, \"drive\" vient de l\\'anglais et veut dire voiture.\\nEn France, cet anglicisme est utilisé pour le service de retrait en magasin avec sa voiture.\\nPour indiquer que le service est disponible aux piétons, certaines enseignes utilisent le terme \"drive piéton\", qui est un oxymore.\\nCette distinction existe parce que certaines enseignes, notamment en centre-ville, n\\'ont pas la place pour un parking voiture, et n\\'offre donc que le service piéton. </s>',\n",
       " '<s>[INST] La Game Boy tenía un accesorio que era una cámara. Pero las fotos que sacaba tenían una apariencia muy característica. ¿Cómo puedo conseguir esa apariencia en una imagen que tengo con el GIMP? [/INST] La apariencia característica de las imágenes de la Game Boy se debe en gran parte a la limitación de colores. Para conseguir esta apariencia en GIMP, primero necesitas reducir el número de colores de tu imagen a 4 o menos. Esto se puede hacer en el menú Imagen > Mode > Indexed. A continuación, necesitas aplicar un efecto de pixelización a la imagen. Esto se puede hacer en el menú Filtros > Desenfoque > Pixelizar. Esta combinación de reducción de colores y pixelización debería ayudarte a conseguir la apariencia de una imagen tomada con una Game Boy. </s>',\n",
       " '<s>[INST] Ich möchte für ein Handy-Game den Flug von Papierfliegern simulieren. Was für Formeln können mir helfen um das möglichst realistisch zu tun? [/INST] Um den Flug von Papierfliegern in einem Spiel realistisch zu simulieren, gibt es einige wichtige Formeln, die Sie berücksichtigen sollten:\\n\\nLift-Kraft: Der Lift, der einen Papierflieger in der Luft hält, wird durch die Bewegung der Luft über die Oberfläche des Flügels erzeugt. Die Formel zur Berechnung der Lift-Kraft lautet:\\n\\nL = 0.5 * rho * v^2 * A * Cl\\n\\nL = Lift-Kraft\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Flügeloberfläche\\nCl = Auftriebsbeiwert des Flügels\\n\\nLuftwiderstand: Der Luftwiderstand ist die Kraft, die gegen den Flug eines Papierfliegers wirkt und durch die Bewegung der Luft entsteht, wenn sie um das Flugzeug herumströmt. Die Formel zur Berechnung des Luftwiderstands lautet:\\n\\nD = 0.5 * rho * v^2 * A * Cd\\n\\nD = Luftwiderstand\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Querschnittsfläche des Flugzeugs\\nCd = Luftwiderstandsbeiwert des Flugzeugs\\n\\nGravitationskraft: Die Gravitationskraft ist die Kraft, die das Flugzeug zur Erde zieht. Die Formel zur Berechnung der Gravitationskraft lautet:\\n\\nFg = m * g\\n\\nFg = Gravitationskraft\\nm = Masse des Flugzeugs\\ng = Beschleunigung durch die Schwerkraft (9,81 m/s^2)\\n\\nGier, Nick- und Rollwinkel: Um die Flugbewegungen eines Papierfliegers realistisch zu simulieren, müssen Sie auch die Gier-, Nick- und Rollwinkel berücksichtigen. Diese Winkel bestimmen die Ausrichtung des Flugzeugs in der Luft und beeinflussen seine Flugbahn.\\n\\nGierwinkel: Der Gierwinkel beschreibt die Drehung des Flugzeugs um die senkrechte Achse. Es wird normalerweise durch den Ruderausschlag des Seitenruders gesteuert.\\nNickwinkel: Der Nickwinkel beschreibt die Drehung des Flugzeugs um die Querachse. Es wird normalerweise durch den Ruderausschlag des Höhenruders gesteuert.\\nRollwinkel: Der Rollwinkel beschreibt die Drehung des Flugzeugs um die Längsachse. Es wird normalerweise durch den Ruderausschlag des Querruders gesteuert.\\nMit diesen Formeln und den entsprechenden Eingabeparametern (z.B. Flugzeuggröße, Fluggeschwindigkeit, Luftdichte) können Sie den Flug von Papierfliegern in Ihrem Spiel realistisch simulieren. Beachten Sie jedoch, dass auch andere Faktoren wie z.B. Wind und Turbulenzen einen Einfluss auf den Flug haben können, und dass die Simulation möglicherweise aufwendig sein kann. </s><s>[INST] Könntest du mir die Formel bitte als Methoden und C# programmieren, damit ich sie in der Game engine Unity verwenden kann? [/INST] Hier ist eine C#-Klasse, die die Formeln für Lift und Luftwiderstand implementiert. Gravitation ließe sich zwar ebenfalls leicht implementieren, ist in Unity aber bereits eingebaut.\\n\\n```csharp\\nclass PaperPlanePhysics {\\n\\t// Physikalische Konstanten\\n\\tpublic float rho = 1.2041f; // Luftdichte, kg/m³\\n\\t\\n\\t// Parameter des Papierfliegers\\n\\tpublic float A; // Flügeloberfläche, m²\\n\\tpublic float Cl; // Auftriebsbeiwert\\n\\tpublic float Cd; // Luftwiderstandsbeiwert\\n\\t\\n\\t// Auftriebskraft\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cl;\\n\\t}\\n\\t\\n\\t// Luftwiderstand\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cd;\\n\\t}\\n\\t\\n\\t// Resultierende Kräfte als Vektor berechnen\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// Windgeschwindigkeit glatt entlang des Flügels\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Vereinfachung: In der Realität ist die Abhängigkeit vom Winkel zwischen Flügel und Luftgeschwindigkeit komplexer.\\n\\t\\t// Nehmen wir an dass es nur von der Geschwindigkeit gerade nach vorne abhängt.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Vereinfachung: Luftwiderstand wirkt in allen Richtungen, möglicherweise mit unterschiedlichen Faktoren.\\n\\t\\t// Wir berechnen hier nur den Luftwiderstand nach vorne.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n```\\n\\nIn einer Game-Engine wie Unity benötigst du in der Regel einen 2D- oder 3D-Vektor für die Kräfte. Die Methode `ComputeForceVector` ist ein Beispiel dafür wie du die Vektoren berechnen kannst. </s><s>[INST] Ich arbeite international mit anderen Entwicklern zusammen und schreibe deshalb ausschließlich englischen Code. Kannst du die Kommentare bitte übersetzen und etwas aussagekräftigere Variablennamen für die Flügeloberfläche, den Auftrieb und den Luftwiderstand verwenden? [/INST] Gerne! Hier ist der englische Code mit besseren Variablennamen:\\n\\n```cs\\nclass PaperPlanePhysics {\\n\\n\\t// physical constants\\n\\tpublic float rho = 1.2041f; // air density, kg/m³\\n\\t\\n\\t// characteristics of the paper plane\\n\\tpublic float wingSurf; // wing surface, m²\\n\\tpublic float liftCoeff; // lift coefficient\\n\\tpublic float dragCoeff; // drag coefficient\\n\\t\\n\\t// calculate lift force based on velocity v\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * liftCoeff;\\n\\t}\\n\\t\\n\\t// calculate air drag based on velocity v\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * dragCoeff;\\n\\t}\\n\\t\\n\\t// calculate all resulting forces as a vector\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// wind speed along the wing\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Simplification: In reality, the relationship between wing angle and airspeed is much more complex.\\n\\t\\t// We assume that it depends only on the speed straight ahead.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Simplification: Air resistance works in all directions, possibly with different factors.\\n\\t\\t// We only compute the forward drag here.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n``` </s>',\n",
       " '<s>[INST] Quien, cuando y como se descubrió la formula de la teoría de la relatividad especial y general? [/INST] La teoría de la relatividad especial fue descubierta por Albert Einstein en 1905, mientras que la teoría de la relatividad general fue descubierta por Einstein en 1915. Ambos descubrimientos fueron realizados a través de un proceso de reflexión y estudio de la física clásica, la mecánica cuántica y otros sistemas matemáticos. </s>',\n",
       " '<s>[INST] Yendo en bicicleta, ¿de qué forma se tiene que señalizar que giras a la izquierda o a la derecha? ¿Tiene que ser con un brazo en concreto, o puede ser con cualquiera? [/INST] Cuando se gira en bicicleta, es importante señalizar con anticipación a los demás usuarios de la vía para evitar accidentes. Para señalizar un giro a la izquierda, se debe extender el brazo izquierdo horizontalmente hacia el lado, y para señalizar un giro a la derecha, se debe extender el brazo derecho horizontalmente hacia el lado.\\n\\nEs importante utilizar el brazo correspondiente al lado del giro que se va a realizar para que sea más fácilmente identificable para los demás usuarios de la vía. Además, es recomendable mantener el brazo extendido durante unos segundos antes de realizar el giro para dar tiempo a los demás usuarios de la vía para reaccionar y adaptarse a la maniobra. </s><s>[INST] que otra señales tengo que hacer al andar en bicicleta [/INST] Además de señalizar cuando se gira, existen otras señales importantes que debes hacer al andar en bicicleta para asegurarte de que los demás usuarios de la vía puedan anticipar tus movimientos y evitar posibles colisiones. Algunas de las señales más importantes son:\\n\\n1. Frenar: Es importante señalar cuando vas a frenar, especialmente si vas a hacerlo bruscamente. Esto lo puedes hacer extendiendo tu brazo izquierdo hacia abajo con la palma de la mano abierta.\\n\\n2. Detenerse: Si necesitas detenerte por completo, debes señalarlo también. Esto lo puedes hacer extendiendo tu brazo izquierdo hacia abajo con la palma de la mano abierta y con los dedos hacia abajo.\\n\\n3. Cambiar de carril: Si necesitas cambiar de carril, es importante señalarlo con antelación. Esto lo puedes hacer extendiendo tu brazo izquierdo hacia la izquierda o hacia la derecha, dependiendo del carril al que te vayas a cambiar.\\n\\n4. Advertir obstáculos: Si hay un obstáculo en el camino, como un bache o un objeto en la vía, es importante señalarlo para que los demás ciclistas puedan evitarlo. Esto lo puedes hacer señalando con tu mano hacia abajo y hacia el lado en el que se encuentra el obstáculo.\\n\\n5. Advertir peligro: Si hay un peligro en la vía, como un vehículo estacionado en un lugar peligroso o un accidente, es importante señalarlo para que los demás ciclistas puedan prestar atención. Esto lo puedes hacer señalando con tu mano hacia arriba y hacia el lado en el que se encuentra el peligro.\\n\\nRecuerda que, aunque estas señales son importantes, lo más importante es mantener la atención en el camino y siempre respetar las normas de tránsito. </s>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f913035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=True,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=no,\n",
       "eval_use_gather_object=False,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=True,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=no,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.0002,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results/runs/Dec25_06-43-33_ajaingcpgpu,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=25,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=cosine,\n",
       "max_grad_norm=0.3,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=paged_adamw_32bit,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "parallelism_config=None,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=4,\n",
       "prediction_loss_only=False,\n",
       "project=huggingface,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=None,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=0,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "trackio_space_id=trackio,\n",
       "use_cpu=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.03,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c20398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trl\n",
    "trl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "245981a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning (SFT) parameters\n",
    "# https://huggingface.co/docs/trl/en/sft_trainer#expected-dataset-type-and-format\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27846284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from tensorboardX) (2.0.0)\n",
      "Requirement already satisfied: packaging in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from tensorboardX) (25.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tensorboardX][0m [tensorboardX]\n",
      "\u001b[1A\u001b[2KSuccessfully installed protobuf-6.33.2 tensorboardX-2.6.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ab3e610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__backends',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activate_neftune',\n",
       " '_add_sm_patterns_to_gitignore',\n",
       " '_align_special_tokens',\n",
       " '_created_lr_scheduler',\n",
       " '_deactivate_neftune',\n",
       " '_determine_best_metric',\n",
       " '_evaluate',\n",
       " '_finish_current_push',\n",
       " '_fsdp_qlora_plugin_updates',\n",
       " '_gather_and_numpify',\n",
       " '_get_collator_with_removed_columns',\n",
       " '_get_dataloader',\n",
       " '_get_eval_sampler',\n",
       " '_get_learning_rate',\n",
       " '_get_num_items_in_batch',\n",
       " '_get_output_dir',\n",
       " '_get_train_sampler',\n",
       " '_hp_search_setup',\n",
       " '_inner_training_loop',\n",
       " '_is_attention_mask_causal',\n",
       " '_is_vision_dataset',\n",
       " '_is_vlm',\n",
       " '_issue_warnings_after_load',\n",
       " '_load_best_model',\n",
       " '_load_callback_state',\n",
       " '_load_from_checkpoint',\n",
       " '_load_optimizer_and_scheduler',\n",
       " '_load_rng_state',\n",
       " '_load_scaler',\n",
       " '_loggers_initialized',\n",
       " '_maybe_log_save_evaluate',\n",
       " '_memory_tracker',\n",
       " '_metrics',\n",
       " '_move_model_to_device',\n",
       " '_name',\n",
       " '_nested_gather',\n",
       " '_paper',\n",
       " '_prepare_context_parallel_inputs',\n",
       " '_prepare_dataset',\n",
       " '_prepare_input',\n",
       " '_prepare_inputs',\n",
       " '_push_from_checkpoint',\n",
       " '_remove_unused_columns',\n",
       " '_report_to_hp_search',\n",
       " '_rotate_checkpoints',\n",
       " '_save',\n",
       " '_save_checkpoint',\n",
       " '_save_optimizer_and_scheduler',\n",
       " '_save_rng_state',\n",
       " '_save_scaler',\n",
       " '_save_tpu',\n",
       " '_set_signature_columns_if_needed',\n",
       " '_signature_columns',\n",
       " '_sorted_checkpoints',\n",
       " '_tag_names',\n",
       " '_template_file',\n",
       " '_total_train_tokens',\n",
       " '_train_batch_size',\n",
       " '_tune_save_checkpoint',\n",
       " '_wrap_model',\n",
       " 'accelerator',\n",
       " 'add_callback',\n",
       " 'args',\n",
       " 'autocast_smart_context_manager',\n",
       " 'aux_loss_enabled',\n",
       " 'call_model_init',\n",
       " 'callback_handler',\n",
       " 'can_return_loss',\n",
       " 'compare_trainer_and_checkpoint_args',\n",
       " 'completion_only_loss',\n",
       " 'compute_loss',\n",
       " 'compute_loss_context_manager',\n",
       " 'compute_loss_func',\n",
       " 'compute_metrics',\n",
       " 'control',\n",
       " 'create_accelerator_and_postprocess',\n",
       " 'create_model_card',\n",
       " 'create_optimizer',\n",
       " 'create_optimizer_and_scheduler',\n",
       " 'create_scheduler',\n",
       " 'current_flos',\n",
       " 'data_collator',\n",
       " 'deepspeed',\n",
       " 'eval_dataset',\n",
       " 'evaluate',\n",
       " 'evaluation_loop',\n",
       " 'floating_point_ops',\n",
       " 'gather_function',\n",
       " 'get_batch_samples',\n",
       " 'get_decay_parameter_names',\n",
       " 'get_eval_dataloader',\n",
       " 'get_learning_rates',\n",
       " 'get_num_trainable_parameters',\n",
       " 'get_optimizer_cls_and_kwargs',\n",
       " 'get_optimizer_group',\n",
       " 'get_test_dataloader',\n",
       " 'get_total_train_batch_size',\n",
       " 'get_tp_size',\n",
       " 'get_train_dataloader',\n",
       " 'hp_name',\n",
       " 'hp_search_backend',\n",
       " 'hub_model_id',\n",
       " 'hyperparameter_search',\n",
       " 'init_hf_repo',\n",
       " 'is_deepspeed_enabled',\n",
       " 'is_fsdp_enabled',\n",
       " 'is_fsdp_xla_enabled',\n",
       " 'is_fsdp_xla_v1_enabled',\n",
       " 'is_fsdp_xla_v2_enabled',\n",
       " 'is_in_train',\n",
       " 'is_local_process_zero',\n",
       " 'is_model_parallel',\n",
       " 'is_tp_enabled',\n",
       " 'is_world_process_zero',\n",
       " 'label_names',\n",
       " 'label_smoother',\n",
       " 'log',\n",
       " 'log_metrics',\n",
       " 'lr_scheduler',\n",
       " 'maybe_activation_offload_context',\n",
       " 'metrics_format',\n",
       " 'model',\n",
       " 'model_accepts_loss_kwargs',\n",
       " 'model_init',\n",
       " 'model_wrapped',\n",
       " 'neftune_noise_alpha',\n",
       " 'num_examples',\n",
       " 'num_tokens',\n",
       " 'num_virtual_tokens',\n",
       " 'optimizer',\n",
       " 'optimizer_cls_and_kwargs',\n",
       " 'padding_free',\n",
       " 'place_model_on_device',\n",
       " 'pop_callback',\n",
       " 'predict',\n",
       " 'prediction_loop',\n",
       " 'prediction_step',\n",
       " 'preprocess_logits_for_metrics',\n",
       " 'processing_class',\n",
       " 'propagate_args_to_deepspeed',\n",
       " 'push_to_hub',\n",
       " 'remove_callback',\n",
       " 'save_metrics',\n",
       " 'save_model',\n",
       " 'save_state',\n",
       " 'set_initial_training_values',\n",
       " 'state',\n",
       " 'store_flos',\n",
       " 'tokenizer',\n",
       " 'torch_jit_model_eval',\n",
       " 'train',\n",
       " 'train_dataset',\n",
       " 'training_step',\n",
       " 'use_apex',\n",
       " 'use_cpu_amp']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99d68e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 58.75 MiB is free. Including non-PyTorch memory, this process has 14.51 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 373.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1245\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1153\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_token_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_token_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1153\u001b[0m (loss, outputs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;66;03m# Compute entropy\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_liger_kernel:  \u001b[38;5;66;03m# liger doesn't return logits\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/peft/peft_model.py:1923\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1922\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1934\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:308\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    308\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 309\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:155\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 155\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:555\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    552\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    553\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(quant_state, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacking_format_for_cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:401\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    318\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 58.75 MiB is free. Including non-PyTorch memory, this process has 14.51 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 373.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0747b968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 25 08:28:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   53C    P0             26W /   70W |   14859MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     28108      C   ...onda3/envs/torch_gpu_dna/bin/python      14856MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5739a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bea9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed8c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78a5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc2da684",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
