# Transformers Tutorial

Educational project for learning Transformer architectures and deep learning fundamentals.

## Overview

This repository contains learning resources and implementations for understanding Transformer models, including materials from reputable sources and practical applications.

## Learning Resources

- **Karpathy's NanoGPT**: Character-level language model from Andrej Karpathy
- **3Blue1Brown Videos**: Visual explanations of neural networks and deep learning concepts
- Additional academic and practical resources

## Key Topics Covered

- Attention mechanisms
- Self-attention and multi-head attention
- Positional encoding
- Transformer encoder/decoder architecture
- Training strategies and optimization
- Fine-tuning pre-trained models

## Structure

```
Transformers_Tutorial/
├── nanoGPT/              # Character-level GPT implementation
├── attention/            # Attention mechanism tutorials
├── implementations/      # Practical implementations
└── notes/               # Learning notes and explanations
```

## Getting Started

```bash
# Install dependencies
pip install torch numpy matplotlib

# Run basic examples
python nanoGPT/train.py
```

## References

- [Karpathy - nanoGPT](https://github.com/karpathy/nanoGPT)
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZZJF1-1p)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)

## Learning Outcomes

After completing this tutorial, you should understand:
- How transformers process sequential data
- The role of attention in modern NLP
- How to implement and train basic transformer models
- Practical applications in language modeling and beyond 
