{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be60f380",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Building GPT from scratch from Andrej karthpathy make more series\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "### Building GPT from scratch from Andrej karthpathy make more series\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5bd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163f3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "os.chdir('/mnt/data/projects/.immune/Personal/Transformers_Tutorial/')\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954a16bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])  # print the first 1000 characters to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d2bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters:\", ''.join(chars))\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b703c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# creating a mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8685bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) <built-in method type of Tensor object at 0x7f3df28734c0>\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# lets encode the entire text into the integer representation\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape,data.type)\n",
    "print(data[:1000])  # first 1000 characters encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf27552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and val sets\n",
    "n = int(0.9*len(data))  # first 90% will be\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4d9642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8  # context length for predictions\n",
    "train_data[:block_size+1] ### till 8 it will be train and 9th character is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769b18e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "print(decode(train_data[:block_size+1].tolist()))  # checking if decoding works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516fec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is 'F' the target: 'i'\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is 'Fi' the target: 'r'\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is 'Fir' the target: 's'\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is 'Firs' the target: 't'\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is 'First' the target: ' '\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is 'First ' the target: 'C'\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is 'First C' the target: 'i'\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
      "when input is 'First Ci' the target: 't'\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]  # input\n",
    "y = train_data[1:block_size+1]  # target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # the context grows with t\n",
    "    target = y[t]     # the target is always the next character\n",
    "    print(f'when input is {context} the target: {target}')\n",
    "    print(f\"when input is {decode(context.tolist())!r} the target: {decode([target.tolist()])!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1855f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "y: tensor([47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # parallel sequences processing\n",
    "block_size = 8  # context length for predictions\n",
    "\n",
    "train_data.shape, val_data.shape\n",
    "data = train_data\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "y - torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41f2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is tensor([24]) the target: 43\n",
      "when input is 'L' the target: 'e'\n",
      "when input is tensor([24, 43]) the target: 58\n",
      "when input is 'Le' the target: 't'\n",
      "when input is tensor([24, 43, 58]) the target: 5\n",
      "when input is 'Let' the target: \"'\"\n",
      "when input is tensor([24, 43, 58,  5]) the target: 57\n",
      "when input is \"Let'\" the target: 's'\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target: 1\n",
      "when input is \"Let's\" the target: ' '\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target: 46\n",
      "when input is \"Let's \" the target: 'h'\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target: 43\n",
      "when input is \"Let's h\" the target: 'e'\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target: 39\n",
      "when input is \"Let's he\" the target: 'a'\n",
      "when input is tensor([44]) the target: 53\n",
      "when input is 'f' the target: 'o'\n",
      "when input is tensor([44, 53]) the target: 56\n",
      "when input is 'fo' the target: 'r'\n",
      "when input is tensor([44, 53, 56]) the target: 1\n",
      "when input is 'for' the target: ' '\n",
      "when input is tensor([44, 53, 56,  1]) the target: 58\n",
      "when input is 'for ' the target: 't'\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target: 46\n",
      "when input is 'for t' the target: 'h'\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target: 39\n",
      "when input is 'for th' the target: 'a'\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target: 58\n",
      "when input is 'for tha' the target: 't'\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is 'for that' the target: ' '\n",
      "when input is tensor([52]) the target: 58\n",
      "when input is 'n' the target: 't'\n",
      "when input is tensor([52, 58]) the target: 1\n",
      "when input is 'nt' the target: ' '\n",
      "when input is tensor([52, 58,  1]) the target: 58\n",
      "when input is 'nt ' the target: 't'\n",
      "when input is tensor([52, 58,  1, 58]) the target: 46\n",
      "when input is 'nt t' the target: 'h'\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target: 39\n",
      "when input is 'nt th' the target: 'a'\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target: 58\n",
      "when input is 'nt tha' the target: 't'\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target: 1\n",
      "when input is 'nt that' the target: ' '\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target: 46\n",
      "when input is 'nt that ' the target: 'h'\n",
      "when input is tensor([25]) the target: 17\n",
      "when input is 'M' the target: 'E'\n",
      "when input is tensor([25, 17]) the target: 27\n",
      "when input is 'ME' the target: 'O'\n",
      "when input is tensor([25, 17, 27]) the target: 10\n",
      "when input is 'MEO' the target: ':'\n",
      "when input is tensor([25, 17, 27, 10]) the target: 0\n",
      "when input is 'MEO:' the target: '\\n'\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target: 21\n",
      "when input is 'MEO:\\n' the target: 'I'\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target: 1\n",
      "when input is 'MEO:\\nI' the target: ' '\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target: 54\n",
      "when input is 'MEO:\\nI ' the target: 'p'\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target: 39\n",
      "when input is 'MEO:\\nI p' the target: 'a'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # parallel sequences processing\n",
    "block_size = 8  # context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # random starting indices\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # input sequences\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # target sequences\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context} the target: {target}')\n",
    "        print(f\"when input is {decode(context.tolist())!r} the target: {decode([target.tolist()])!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cebd26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 65)\n",
      ")\n",
      "logits: tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# Constructing the bigram model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # this is random initialization which will be learned during training.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "# print(targets.shape) # (B*T, C)\n",
    "# torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# This is completely random generation as the model is untrained.\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a549fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c1d1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44f47a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6243, -0.1004,  0.2456,  ..., -1.2338,  0.0534, -0.1949],\n",
       "        [ 0.8603, -0.2675, -0.4354,  ..., -0.1845, -1.0497, -0.8261],\n",
       "        [ 1.7343,  0.8752,  1.5813,  ...,  1.2927,  0.7182,  0.6097],\n",
       "        ...,\n",
       "        [-1.6689, -3.3766, -0.4560,  ...,  0.2780, -1.5953, -0.4737],\n",
       "        [-1.2969,  1.8702, -1.0598,  ..., -1.2579,  0.1119,  0.3389],\n",
       "        [-0.0886, -1.0879,  0.9975,  ..., -1.2604,  0.8044, -1.9819]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding=nn.Embedding(vocab_size, vocab_size)\n",
    "token_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a829f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = token_embedding(xb) # (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0592b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3660e-01, -1.8731e+00,  7.1989e-01,  1.3609e-01,  9.8589e-01,\n",
      "          1.5483e+00, -2.6414e-01, -1.1776e+00,  5.1714e-01, -4.6053e-01,\n",
      "          4.2017e-01, -9.6025e-01,  1.3287e+00,  2.0909e-01,  8.6215e-01,\n",
      "          4.3085e-02,  5.7950e-01, -2.0212e+00, -8.1185e-01,  6.5908e-01,\n",
      "         -7.5360e-02,  1.3957e+00,  2.0643e+00, -2.0412e+00, -3.7620e-01,\n",
      "         -3.8499e-01,  3.5262e-01, -4.5423e-01,  1.1916e+00, -8.5033e-01,\n",
      "         -1.1318e+00,  9.0647e-01,  5.6884e-01, -2.9375e+00,  4.6553e-01,\n",
      "         -9.6770e-01, -1.4652e+00, -2.6071e-01, -1.0342e+00, -1.3620e+00,\n",
      "         -1.2595e+00, -1.8476e+00,  1.1339e+00, -2.0390e-01,  2.8299e-01,\n",
      "          3.1425e-01,  1.5377e+00,  9.7995e-02,  1.5568e-01, -3.9144e-01,\n",
      "         -1.0180e+00,  2.9127e-01, -1.4972e+00,  8.3221e-01,  1.5373e+00,\n",
      "         -1.1809e+00,  2.8087e+00,  1.7374e+00, -5.5859e-01, -8.0195e-01,\n",
      "         -5.1946e-01, -9.9884e-01,  2.1045e-01,  8.7487e-01,  2.4380e-01],\n",
      "        [ 4.1719e-01,  6.3408e-01, -1.8520e+00,  1.3329e+00, -2.3835e-01,\n",
      "         -8.3095e-01,  3.6281e-01,  2.8096e-01,  4.8274e-01,  1.9110e+00,\n",
      "         -1.3013e-01, -1.1725e+00, -6.3693e-01, -1.8083e-01, -8.8123e-01,\n",
      "          7.9419e-01,  1.5290e-01,  1.3080e+00, -6.3439e-01,  9.1087e-01,\n",
      "          9.4972e-01, -1.6114e+00,  8.5771e-02,  1.7488e+00,  1.3325e+00,\n",
      "         -9.7220e-01,  1.6579e-01,  1.0346e+00,  1.0876e-02,  1.3998e-01,\n",
      "          2.1353e+00,  1.2158e+00, -1.5107e+00,  7.4399e-02,  4.2628e-01,\n",
      "          3.6319e-01,  7.7939e-02,  7.5884e-01, -1.3059e+00, -6.2333e-01,\n",
      "         -1.1189e+00,  1.1120e+00,  1.2642e+00,  5.3649e-01,  4.0709e-02,\n",
      "          1.3487e+00,  8.6895e-01, -9.8258e-01,  6.4742e-01,  1.9123e-01,\n",
      "          6.9911e-02,  1.4409e+00,  9.1732e-02, -1.8625e+00, -1.0470e+00,\n",
      "          6.6066e-01, -3.2875e-02,  2.4735e-01,  4.1963e-01,  1.9724e+00,\n",
      "          1.2799e+00,  1.0090e+00, -2.4700e-01, -2.6345e-01, -3.2457e-01],\n",
      "        [ 2.0899e+00, -5.2521e-01, -4.6966e-01, -5.4845e-01,  2.8646e-01,\n",
      "         -7.2916e-01,  6.7663e-01,  5.5633e-01,  8.5716e-01,  1.3767e+00,\n",
      "          4.0655e-01,  9.2928e-01, -2.7701e-01, -3.3431e-01, -8.4452e-01,\n",
      "          4.0344e-01,  6.5026e-01,  2.7022e-01,  3.0861e-01, -2.7157e-01,\n",
      "         -2.8080e+00, -1.6318e+00,  1.0151e-01,  1.5565e+00,  1.2029e+00,\n",
      "          1.5426e+00, -1.8607e-01,  2.1844e-01,  1.0582e+00, -2.2981e-02,\n",
      "          4.4055e-01,  1.3465e+00, -6.4695e-01,  9.9397e-01,  5.2276e-01,\n",
      "          9.9946e-01,  9.7238e-01, -2.0587e+00, -1.2321e+00,  4.3203e-01,\n",
      "         -3.1966e-01, -1.7550e+00, -2.6497e-01,  4.9805e-01,  5.4433e-01,\n",
      "          8.4292e-01,  5.7435e-02,  3.1024e-01, -1.8794e-01,  4.1479e-01,\n",
      "         -2.0570e-01,  1.3358e+00, -1.8638e+00,  1.5129e+00, -1.7818e+00,\n",
      "         -4.6854e-03,  1.7688e+00, -1.0526e-01,  5.9094e-02, -1.3655e+00,\n",
      "          1.3215e+00,  2.6359e-01,  1.4875e+00,  6.6250e-01,  2.5883e+00],\n",
      "        [ 8.3439e-01,  4.8268e-01,  3.2132e-01,  9.8181e-01,  1.1415e+00,\n",
      "          4.6572e-01, -1.2951e+00,  3.4767e-01, -2.0663e+00,  4.9599e-01,\n",
      "         -4.0669e-02, -2.9222e+00, -2.8413e-01,  8.8407e-02, -9.8454e-01,\n",
      "          1.6508e+00, -9.0351e-01, -1.6502e+00, -9.8214e-01,  1.4338e+00,\n",
      "          1.6344e-01, -2.3246e+00, -6.6426e-01,  3.4425e-01,  2.5640e-01,\n",
      "         -1.5011e+00, -1.2276e-01, -1.5234e-01, -7.4305e-01, -1.0599e+00,\n",
      "         -5.3123e-01,  1.1549e+00,  1.3081e+00,  7.3786e-01, -1.6591e+00,\n",
      "         -8.4010e-01, -1.3581e+00,  4.3531e-01, -1.9206e+00,  1.6105e+00,\n",
      "          2.4026e-01, -8.6997e-02, -1.1245e+00, -8.3828e-01,  1.8211e-01,\n",
      "         -2.8415e-01,  9.7605e-01,  2.0496e+00,  1.2779e+00, -5.9809e-01,\n",
      "         -2.0872e-01, -1.8325e+00, -1.8434e-02, -8.9431e-01, -2.6908e-01,\n",
      "          7.7620e-02, -4.9254e-01, -5.6568e-02,  4.7034e-01, -8.4867e-02,\n",
      "         -1.0538e+00,  1.0298e+00, -6.5059e-01,  5.3971e-01, -4.6932e-01],\n",
      "        [ 1.7496e+00, -1.3142e+00, -2.6818e-01, -1.6090e-01,  7.6331e-01,\n",
      "          9.6112e-01, -1.5080e-01, -3.1125e-01, -2.3261e-01, -5.8817e-01,\n",
      "          1.4177e+00, -1.2886e+00,  6.5825e-01, -1.4660e+00, -2.5495e-01,\n",
      "         -1.5867e+00, -1.3033e-01,  2.6345e-01,  8.0033e-01, -2.8147e-01,\n",
      "          4.9364e-01, -1.2931e-01,  1.1039e+00,  2.0847e-01,  3.4679e-01,\n",
      "         -5.4988e-01,  6.4344e-01,  6.0704e-01,  1.3066e+00,  1.8688e+00,\n",
      "         -2.2867e-01, -4.9845e-01,  9.7364e-01,  4.2643e-04, -1.6465e-01,\n",
      "         -5.3298e-01,  8.4407e-01, -1.4206e+00,  1.6876e-01,  3.8585e-01,\n",
      "          6.9129e-01,  4.2996e-01,  9.9014e-01, -7.2189e-01, -9.9551e-01,\n",
      "          8.5943e-01,  1.5289e+00,  1.5251e+00,  1.4120e-01,  1.5508e+00,\n",
      "         -2.6785e-01,  8.5284e-01, -2.0275e+00, -6.9661e-01, -1.5299e+00,\n",
      "          6.5047e-01, -5.7164e-01, -2.4566e-01,  5.7103e-01, -3.7924e-01,\n",
      "          8.6407e-01,  6.0657e-01, -2.9749e-01,  9.4387e-01, -2.1673e+00],\n",
      "        [ 8.6031e-01, -2.6748e-01, -4.3536e-01,  3.9767e-01, -1.5627e-01,\n",
      "         -2.8093e-02,  1.7915e+00, -3.8532e-01,  7.4799e-01,  1.1025e+00,\n",
      "         -1.7147e-01,  4.5868e-01, -2.6170e+00, -1.8637e+00,  1.6446e+00,\n",
      "         -1.4193e+00,  6.3701e-01, -2.0427e+00,  2.6179e-01, -3.8560e-01,\n",
      "         -1.3818e+00,  1.5872e-01,  3.5286e-01,  1.2170e+00, -1.0035e+00,\n",
      "         -5.6401e-01, -1.3432e+00, -5.3805e-01,  7.8276e-01, -9.2205e-01,\n",
      "         -8.4715e-01,  8.3337e-01,  2.1029e-01, -2.3446e-01, -1.3996e-02,\n",
      "          1.0262e+00, -8.9598e-01, -9.6618e-01,  1.0666e+00,  1.3343e+00,\n",
      "         -1.5190e-01,  3.9288e-01,  7.0952e-01,  3.2068e-01, -4.2619e-01,\n",
      "         -6.5384e-02,  1.2530e+00,  2.4880e-01,  7.9830e-01, -1.0938e+00,\n",
      "         -1.1291e-01,  9.6340e-01, -3.0237e-01, -5.6314e-01, -1.3439e-01,\n",
      "         -1.0981e+00,  1.4837e-01, -1.2356e-01, -1.3368e-01, -4.3396e-01,\n",
      "          7.8065e-01,  1.0760e+00, -1.8449e-01, -1.0497e+00, -8.2610e-01],\n",
      "        [-5.5028e-01,  2.4553e-01, -8.1050e-01,  8.0002e-02,  1.6700e+00,\n",
      "         -1.5319e+00,  7.8733e-01,  1.3545e+00,  5.2466e-01,  5.0678e-01,\n",
      "         -8.8718e-01,  7.7248e-01, -6.5893e-02, -5.7829e-01,  3.5940e-01,\n",
      "         -5.0790e-01,  4.0299e-01,  5.2060e-01, -6.6829e-01, -8.8536e-01,\n",
      "          2.8420e-01, -8.6382e-02, -1.2906e+00,  1.1906e+00,  7.8378e-01,\n",
      "         -1.2473e-02, -1.8007e+00, -1.2373e+00,  1.2439e+00,  7.6467e-01,\n",
      "          2.5281e-02, -9.4156e-01,  1.2521e-01,  9.0356e-01, -1.8441e-01,\n",
      "          6.2462e-01, -1.4374e+00, -9.6756e-02,  1.4749e+00,  6.9119e-01,\n",
      "          1.1509e+00, -8.0330e-01,  4.3955e-01, -1.0593e+00,  1.0395e+00,\n",
      "          1.4556e+00,  3.8041e-01, -4.2485e-01,  3.2582e-01,  9.9553e-01,\n",
      "         -3.1884e-01,  1.1425e+00, -7.0066e-01,  1.1699e+00,  4.0563e-01,\n",
      "          3.9237e-01,  3.9008e-01,  8.0120e-01, -1.1087e+00, -1.5223e-01,\n",
      "         -3.5908e-01,  1.2743e+00, -4.7150e-01,  8.0781e-01, -5.7455e-01],\n",
      "        [ 4.1719e-01,  6.3408e-01, -1.8520e+00,  1.3329e+00, -2.3835e-01,\n",
      "         -8.3095e-01,  3.6281e-01,  2.8096e-01,  4.8274e-01,  1.9110e+00,\n",
      "         -1.3013e-01, -1.1725e+00, -6.3693e-01, -1.8083e-01, -8.8123e-01,\n",
      "          7.9419e-01,  1.5290e-01,  1.3080e+00, -6.3439e-01,  9.1087e-01,\n",
      "          9.4972e-01, -1.6114e+00,  8.5771e-02,  1.7488e+00,  1.3325e+00,\n",
      "         -9.7220e-01,  1.6579e-01,  1.0346e+00,  1.0876e-02,  1.3998e-01,\n",
      "          2.1353e+00,  1.2158e+00, -1.5107e+00,  7.4399e-02,  4.2628e-01,\n",
      "          3.6319e-01,  7.7939e-02,  7.5884e-01, -1.3059e+00, -6.2333e-01,\n",
      "         -1.1189e+00,  1.1120e+00,  1.2642e+00,  5.3649e-01,  4.0709e-02,\n",
      "          1.3487e+00,  8.6895e-01, -9.8258e-01,  6.4742e-01,  1.9123e-01,\n",
      "          6.9911e-02,  1.4409e+00,  9.1732e-02, -1.8625e+00, -1.0470e+00,\n",
      "          6.6066e-01, -3.2875e-02,  2.4735e-01,  4.1963e-01,  1.9724e+00,\n",
      "          1.2799e+00,  1.0090e+00, -2.4700e-01, -2.6345e-01, -3.2457e-01]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 8])\n",
      "torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(logits[0])\n",
    "print(loss)\n",
    "print(xb.shape) # B X T\n",
    "print(yb.shape) # B X T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e376e",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b39d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6904830932617188\n"
     ]
    }
   ],
   "source": [
    "### This is mainly learning from the probability of the look up table\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    # optimize the model\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34df302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ss d tcI:\n",
      "AUNorthiref s is, XLANzncouso;AUCl\n",
      "AHllico ithen, Horeo t had,\n",
      "\n",
      "Mh\n",
      "FINolly\n",
      "\n",
      "\n",
      "\n",
      "Uy ou ttirve\n",
      "' ll ato, s then w; s w\n",
      "EQzL:CEXR:\n",
      "Whas nsu bfof; cond,\n",
      "\n",
      "IFtrune e fo wa at we al\n",
      "Cid thind ad cas;\n",
      "Youtod,\n",
      "Fo s mp aSwhy'lout.\n",
      "RFXWathininoth itethe t r; terus rs t;\n",
      "vell COUCENET:\n",
      "bints tes?$a:\n",
      "OLk'douid ours;\n",
      "p' w'raver:\n",
      "\n",
      "e,\n",
      "AUA c,\n",
      "UcousexFIEOr orous sthVIOLinr om!\n",
      "STEDICHe kenf tis bau d IU horf t q.\n",
      "bur,\n",
      "Issthe; rinodvely,V&ven tang, horche.?\n",
      "Thaceayousomege, thage pe'se.\n",
      "ORYQ!ve\n",
      "JFVbep On s\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217cab2c",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f2c0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "## This is for 2 dimesnional matrix multiplication\n",
    "# Since in transformer we will be using 2D matrices for attention calculations.\n",
    "# In attention we use only those character before the current character for prediction.\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) # Lower triangular matrix\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # scaling the rows to sum to 1\n",
    "print(a)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(b)\n",
    "c = a @ b\n",
    "print(c) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b2f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 3])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "## This is for 3 dimesnional matrix multiplication\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,4,3)) # Lower triangular matrix\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # scaling the rows to sum to 1\n",
    "# print(a)\n",
    "print(a.shape)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "# print(b)\n",
    "print(b.shape)\n",
    "c = a @ b\n",
    "# print(c)\n",
    "print(c.shape)\n",
    "# so regarding the dimensions, a = (3,4,3) and b = (3,2) results in c = (3,4,2)\n",
    "# since b = (3,3,2) and while matrix multiplication the last two dimensions are considered for multiplication.\n",
    "# so a = (4,3) and b = (3,2) results in c = (4,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f477f1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n"
     ]
    }
   ],
   "source": [
    "# try out on our dataset\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be360058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "### In the self attention here we are just averaging the previous tokens to get a sense of context.\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "print(xbow.shape)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev,0) # averaging the previous tokens including the current token\n",
    "print(xbow.shape)\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61dac0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.8173,  0.4127],\n",
      "        [-0.1342,  0.4395],\n",
      "        [ 0.2711,  0.4774],\n",
      "        [ 0.2421,  0.0694],\n",
      "        [ 0.0084,  0.0020],\n",
      "        [ 0.0712, -0.1128],\n",
      "        [ 0.2527,  0.2149]])\n",
      "tensor([[ 1.3488, -0.1396],\n",
      "        [ 0.8173,  0.4127],\n",
      "        [-0.1342,  0.4395],\n",
      "        [ 0.2711,  0.4774],\n",
      "        [ 0.2421,  0.0694],\n",
      "        [ 0.0084,  0.0020],\n",
      "        [ 0.0712, -0.1128],\n",
      "        [ 0.2527,  0.2149]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Another more efficient way\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)  #\n",
    "print(wei)\n",
    "xbow2 = wei @ x  # (T,T) @ (T,C) -->\n",
    "print(xbow2.shape)\n",
    "print(xbow2[1])\n",
    "print(xbow[1])\n",
    "torch.allclose(xbow[0], xbow2[0]) ### it is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2fc2ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8a940e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd99d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### how much does the previous elements contribute to the current element. \n",
    "# so we can use the softmax which is kind of the normalization\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f115651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "logits: tensor([[ 0.6532, -0.5768,  0.1623,  ...,  0.9086, -0.3203, -0.5369],\n",
      "        [ 0.4420,  0.1231,  0.8807,  ...,  0.5617,  0.1265,  0.2913],\n",
      "        [ 1.0582, -0.5045,  0.3036,  ..., -0.3142,  0.5471,  0.7962],\n",
      "        ...,\n",
      "        [-0.1580, -0.4475,  0.3257,  ...,  0.1921,  0.7424,  0.4570],\n",
      "        [ 0.4546, -0.3635,  0.2983,  ...,  0.2652,  0.5731,  0.0840],\n",
      "        [-0.6873, -0.4842,  0.6730,  ..., -0.5416, -0.7946, -0.7934]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.2468, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "lN!BJ'kysLCMFJPKOL?DP-QWwrEoL?jLDJQOL.f'RIHD'Hdhs Yv,wxatnscMZwtEOS'palkq3ssZeAvzF-QT;eMk;x.gQSFCLgx\n"
     ]
    }
   ],
   "source": [
    "# Constructing the bigram model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        logits = self.lm_head(token_emb)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 32\n",
    "batch_size = 32\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "# print(targets.shape) # (B*T, C)\n",
    "# torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# This is completely random generation as the model is untrained.\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2a13cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a3b9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5342628955841064\n"
     ]
    }
   ],
   "source": [
    "## Training a model\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c763db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Th thalldy hangilyoteng h hasbe pan hatrance\n",
      "RDe hicomyonthar's\n",
      "PES:\n",
      "AKEd ith henoungincenonthiousir thondy, y heltieiengerofo'dsssit ey\n",
      "KINld pe wither vouprrouthercckeha t,\n",
      "K:\n",
      "\n",
      "My hind tt hinig t ouchos tes; st younind wotte grotonear 'so it t jod weancothanan hay. t--s n prids, r loncave w hollular e O:\n",
      "HIs; ht anje caike ineent.\n",
      "\n",
      "Lavinde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, aridesar, wyonthenous aves, theresseys\n",
      "Plorseelapinghiybend yof GLANCHI me. strsithisgothers w de o! ABe\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a356b",
   "metadata": {},
   "source": [
    "#### Adding the Position Embedding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ea429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27ec4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "## First lets initialize \n",
    "vocab_size = 65 # since in the Shakespeare vocabulary has 65 characters \n",
    "emb_size = 128 # total number of dimensions\n",
    "batch_size = 32 # batch size\n",
    "token_size = 8\n",
    "## It is just randomly creating an embedding for all the vocabulary and Tokens with 128 dimensions\n",
    "token_embedding_table = nn.Embedding(vocab_size, emb_size) \n",
    "token_position_table = nn.Embedding(token_size, emb_size)\n",
    "lm_head = nn.Linear(emb_size, vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abfa9a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character embedding\n",
      "Embedding(65, 128)\n",
      "torch.Size([128])\n",
      "position embedding\n",
      "Embedding(8, 128)\n",
      "torch.Size([128])\n",
      "linear Model\n",
      "Linear(in_features=128, out_features=65, bias=True)\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"character embedding\")\n",
    "print(token_embedding_table)\n",
    "print(token_embedding_table.weight[0,:].shape)\n",
    "# print(token_embedding_table.weight[0,:] )\n",
    "# This shows for first character what is the weight will keep on updating during the backpropogation\n",
    "print(\"position embedding\")\n",
    "print(token_position_table)\n",
    "print(token_position_table.weight[0,:].shape)\n",
    "# print(token_position_table.weight[0,:])\n",
    "# This shows for first token position what is the weight will keep on updating during the backpropogation\n",
    "print(\"linear Model\")\n",
    "print(lm_head)\n",
    "print(lm_head.weight[0,:].shape)\n",
    "# lm_head.weight[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25ffd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "        token_embedding_table.weight,\n",
    "        token_position_table.weight,\n",
    "        lm_head.weight,\n",
    "        lm_head.bias\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba422fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.400627851486206\n"
     ]
    }
   ],
   "source": [
    "### After initialization, we will input the values\n",
    "# print(\"Input=\",xb[0:1:10], xb.shape,\"\\nTarget=\",yb[0:1:10], yb.shape)\n",
    "# Now we have to embed these tokens\n",
    "# token_character embedding which will provide us Batch, Token, Embedding 32, 8, 128 (B,T,C)\n",
    "\n",
    "for steps in range(10000):\n",
    "    B,T = xb.shape\n",
    "    token_emb= token_embedding_table(xb)\n",
    "    # print(\"Input_embdding=\",token_emb.shape)\n",
    "    position_emb= token_position_table(torch.arange(T)) ## Since we have to take care of the position\n",
    "    # print(\"Input_pos_embdding=\",position_emb.shape)\n",
    "    x = token_emb + position_emb\n",
    "    # print(\"x=\",x.shape)\n",
    "    logits = lm_head(x) # xW.transpose() + bias\n",
    "    B,T,C = logits.shape\n",
    "    logits3 = logits.view(B*T,C)\n",
    "    # print(\"logits:\",logits3.shape)\n",
    "    targets = yb.view(B*T)\n",
    "    # print('targets:',targets.shape)\n",
    "    loss = F.cross_entropy(logits3, targets)\n",
    "    # optimizer.zero_grad(set_to_none = True)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # Gradient reset\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    lr = 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    # logits2 = logits[:,-1,:]\n",
    "    # print(\"logits=\",logits2.shape)\n",
    "    # prob = F.softmax(logits2, dim = -1)\n",
    "    # print(\"prob=\",prob.shape)\n",
    "    # print(prob.sum(1))\n",
    "\n",
    "print(loss.item()) # since we are training on single batch so it is overcorrecting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73b765f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    # idx: (B, T) tensor of token indices (context). Usually start with [[0]].\n",
    "    for _ in range(max_new_tokens):\n",
    "        B, T = idx.shape\n",
    "        # Forward pass for only the current context\n",
    "        token_emb = token_embedding_table(idx)                         # (B, T, C)\n",
    "        position_emb = token_position_table(torch.arange(T)) # (T, C)\n",
    "        x = token_emb + position_emb                                   # (B, T, C)\n",
    "        logits = lm_head(x)                                            # (B, T, vocab)\n",
    "        # take only the last time step\n",
    "        logits = logits[:, -1, :]                                      # (B, vocab)\n",
    "        # convert logits  probabilities\n",
    "        probs = F.softmax(logits, dim=-1)                              # (B, vocab)\n",
    "        # sample next token\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)             # (B, 1)\n",
    "        # append to sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                        # (B, T+1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc9be9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". ,aIm,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros((1, 1), dtype=torch.long)   # context = \"unknown token\" / start token\n",
    "output = generate(start, max_new_tokens=8)\n",
    "print(decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bca05600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 23, 13, 32, 20, 13,  1, 21,  1, 61, 47, 49, 43, 57,  1, 61, 47, 49,\n",
      "         43,  1, 44, 43,  1, 53, 51, 22, 53, 51, 59, 56, 19, 53, 19, 53, 44, 44,\n",
      "         39, 52, 58, 39, 52, 58, 46, 47, 58, 58, 39, 52, 54,  4, 26, 26, 53, 61,\n",
      "         53, 51, 39, 52, 60, 43, 52, 58, 46, 43, 52, 58, 39,  0, 19, 19, 53, 20,\n",
      "         53, 51, 39, 49, 43,  1, 26,  1, 63, 36, 51, 39, 52, 47, 53, 56, 47, 49,\n",
      "         43, 19, 53, 44,  1, 51, 51, 39, 52, 42,  1, 46, 47, 41, 46, 47, 49, 43,\n",
      "          1, 41, 47, 49, 43, 23,  1, 26, 39, 52, 45, 47, 57, 47, 53, 58, 46, 47,\n",
      "         49, 43,  1, 26, 39, 52, 53, 44, 44, 47, 49, 43, 56, 41, 47, 57,  1, 51,\n",
      "         41, 47, 49, 43,  1, 61, 47, 49, 43, 62, 22, 44, 41, 59, 54, 47, 39, 47,\n",
      "         54, 64, 43,  1, 63,  1, 26, 39, 52, 54, 53, 51, 41, 46, 47, 58, 46, 47,\n",
      "         49, 43,  1, 46, 11, 57,  1, 51, 39, 53, 61, 47,  1, 62, 52, 42,  1, 63,\n",
      "         44, 44, 21]])\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)  # or start token / first character\n",
    "generated = context.clone()\n",
    "\n",
    "for _ in range(200):   # generate 200 characters\n",
    "    # crop context to last 8 chars (model's block size)\n",
    "    context_cropped = generated[:, -8:]\n",
    "\n",
    "    token_emb = token_embedding_table(context_cropped)\n",
    "    pos_emb = token_position_table(torch.arange(context_cropped.shape[1]))\n",
    "    x = token_emb + pos_emb\n",
    "    logits = lm_head(x)\n",
    "\n",
    "    # take logits of last time step\n",
    "    logits_last = logits[:, -1, :]\n",
    "    probs = F.softmax(logits_last, dim=-1)\n",
    "\n",
    "    # sample from distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    # append to generated sequence\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1ffd06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KATHA I wikes wike fe omJomurGoGoffantanthittanp&NNowomanventhenta\n",
      "GGoHomake N yXmaniorikeGof mmand hichike cikeK Nangisiothike Nanoffikercis mcike wikexJfcupiaipze y Nanpomchithike h;s maowi xnd yffI\n"
     ]
    }
   ],
   "source": [
    "decoded = ''.join([itos[int(i)] for i in generated[0].tolist()])\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef2d9ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2227048873901367\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    B, T = xb.shape\n",
    "    token_emb = token_embedding_table(xb)                        # (B, T, C)\n",
    "    position_emb = token_position_table(torch.arange(T))  # (T, C)\n",
    "    x = token_emb + position_emb                                 # (B, T, C)\n",
    "\n",
    "    logits = lm_head(x)                                          # (B, T, vocab)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = yb.view(B*T)\n",
    "\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # Gradient reset\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    lr = 1e-3\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abbc3950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5143752098083496\n"
     ]
    }
   ],
   "source": [
    "## Training a model\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1c421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ea4caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (token_position_embedding_table): Embedding(8, 32)\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "logits: tensor([[ 0.1846, -0.1045, -1.0437,  ..., -0.4779,  0.1244,  0.5709],\n",
      "        [ 0.5446,  0.3983,  0.2476,  ...,  0.3400,  0.5947, -0.8438],\n",
      "        [ 0.4003,  1.3123,  1.5617,  ...,  0.2886, -0.7267,  0.4469],\n",
      "        ...,\n",
      "        [ 1.0967,  0.7229,  0.1110,  ...,  0.0170,  1.9136,  0.7253],\n",
      "        [ 0.5252, -0.7673, -0.1252,  ..., -0.2411,  0.9101,  1.4005],\n",
      "        [-0.6442, -0.5236,  1.1791,  ..., -0.7567, -0.3258,  0.3364]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.4711, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([256, 65])\n"
     ]
    }
   ],
   "source": [
    "# Constructing the bigram model\n",
    "## In the Bigram model it does not really matter the position since you will only be contructing or predicting based on the one character before\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.token_position_embedding_table = nn.Embedding(block_size, n_emb) # position embedding of one batch in the context\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        B,T = idx.shape # B-Batch, T-Tokens, C-Channel i.e. dimensions\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C) (32,8,32)\n",
    "        position_emb = self.token_position_embedding_table(torch.arange(T)) ## for each block is arrange as 0,1,2,3\n",
    "        x = token_emb + position_emb\n",
    "        logits = self.lm_head(x) # xW.transpose() + b \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # cropping the idx so that in the position embedding it is not bigger than the expecting so we will crop it\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 32 # this is basically the dimensions size\n",
    "batch_size = 32\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "# print(targets.shape) # (B*T, C)\n",
    "# torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# This is completely random generation as the model is untrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30b825a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: Linear(in_features=32, out_features=16, bias=False)\n",
      "key weight: torch.Size([16, 32])\n",
      "query: Linear(in_features=32, out_features=16, bias=False)\n",
      "query weight: torch.Size([16, 32])\n",
      "value: Linear(in_features=32, out_features=16, bias=False)\n",
      "value weight: torch.Size([16, 32])\n",
      "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
      "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
      "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "wei: torch.Size([4, 8, 8])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "wei: torch.Size([4, 8, 8])\n",
      "k shape: torch.Size([4, 8, 16])\n",
      "q shape: torch.Size([4, 8, 16])\n",
      "v shape: torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "## Self Attention Mechanism, we require key, query, and values\n",
    "# attention = softmax((q @ wT)/d**0.5) @ v\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch tokens channels (dimension)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16 # it means we have 2 heads 32 / 16 = 2\n",
    "\n",
    "key = nn.Linear(C, head_size, bias = False) # C is the in-features and head_size = out_features\n",
    "query = nn.Linear(C, head_size, bias = False) # key\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "print(\"key:\", key)\n",
    "print(\"key weight:\", key.weight.shape)\n",
    "print(\"query:\", query)\n",
    "print(\"query weight:\", query.weight.shape)\n",
    "print(\"value:\", value)\n",
    "print(\"value weight:\", value.weight.shape)\n",
    "\n",
    "k = key(x) # B,16,T # x @ W.transpose Linear layer, where W has dimension C, head_size\n",
    "q = query(x) # B,16,T\n",
    "\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "print(wei[0])\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei[0])\n",
    "print(\"wei:\",wei.shape)\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "print(wei[0])\n",
    "print(\"wei:\",wei.shape)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"q shape:\", q.shape)\n",
    "print(\"v shape:\", v.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a02184",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a head module\n",
    "# attention = softmax((q @ wT)/d**0.5)) @ v\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        self.key = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        self.value = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        # here tril is not a paramter it is a buffer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        # self.head_size = head_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # B,T,C\n",
    "        q = self.query(x) # B,H,T\n",
    "        k = self.key(x) # B,H,T\n",
    "        # compute attention score\n",
    "        # ((q @ wT)/d**0.5))\n",
    "        # wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # B,T,H @ B,H,T --> B,T,T\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # B,T,H @ B,H,T --> B,T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1) # softmax((q @ wT)/d**0.5))\n",
    "        v = self.value(x) \n",
    "        out = wei @ v # softmax((q @ wT)/d**0.5)) @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188645f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ba47816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (token_position_embedding_table): Embedding(8, 32)\n",
      "  (sa_head): Head(\n",
      "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "logits: tensor([[ 0.2431,  0.0681, -0.3286,  ...,  0.2470, -0.2253,  0.0566],\n",
      "        [-0.1222,  0.0324, -0.1054,  ...,  0.0134, -0.7218,  0.0294],\n",
      "        [-0.0980, -0.0212, -0.0708,  ..., -0.1384, -0.6062, -0.0994],\n",
      "        ...,\n",
      "        [-0.3403, -0.0469,  0.3340,  ...,  0.0522, -0.5408, -0.0759],\n",
      "        [-0.1054,  0.1624,  0.0327,  ..., -0.2002,  0.1835, -0.0298],\n",
      "        [-0.0480,  0.1829, -0.1359,  ..., -0.4397,  0.2710, -0.0786]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.1673, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([256, 65])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Now we will add the self attention head to this bigram model\n",
    "# Constructing the bigram model\n",
    "## In the Bigram model it does not really matter the position since you will only be contructing or predicting based on the one character before\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.token_position_embedding_table = nn.Embedding(block_size, n_emb) # position embedding of one batch in the context\n",
    "        self.sa_head = Head(head_size)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        B,T = idx.shape # B-Batch, T-Tokens, C-Channel i.e. dimensions\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C) (32,8,32)\n",
    "        position_emb = self.token_position_embedding_table(torch.arange(T)) ## for each block is arrange as 0,1,2,3\n",
    "        x = token_emb + position_emb\n",
    "        x = self.sa_head(x) \n",
    "        logits = self.lm_head(x) # xW.transpose() + b \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # cropping the idx so that in the position embedding it is not bigger than the expecting so we will crop it\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_crop) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 32 # this is basically the dimensions size\n",
    "batch_size = 32\n",
    "head_size = 32\n",
    "block_size = 8\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "print(targets.shape) # (B*T, C)\n",
    "# # torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# # This is completely random generation as the model is untrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c54518bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3578009605407715\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "## Training a model\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc046434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hadd wist. Be sothereanomman in etalll illd? fisot\n",
      "Th lal theise,\n",
      "pit ban, pray ind, cikigelu inecast afrro nodess\n",
      "\n",
      "By nee ive nno bou mad'des twhep'y st bazel\n",
      "TELONO:\n",
      "Y d''Teath n'tsous migaer wofr woudl as lisen, hisee,\n",
      "Keratoh e'ld, his ssn nide te! as-'lly sin chof thes 'dis asw ht honger vit oul you,\n",
      "Lonccouroust,\n",
      "G ga oblleandalelarminthaf Thont pare nd heand ay out sitout youn havegnellfor,\n",
      "Ay to bol my, a my dsht muden ise sesinot imeadeags.\n",
      "\n",
      "CERLe Ine ows seewer, yousten do fothedst wak\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0919ff1",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ac9174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9915d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (token_position_embedding_table): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "logits: tensor([[ 0.7121, -0.0878, -0.2714,  ...,  0.0534,  0.3479, -0.1187],\n",
      "        [ 0.0012, -0.2142, -0.0085,  ..., -0.0389, -0.0339, -0.0025],\n",
      "        [-0.0431, -0.3961,  0.0912,  ..., -0.3318,  0.1192, -0.1161],\n",
      "        ...,\n",
      "        [-0.2010,  0.3399, -0.2436,  ...,  0.1715,  0.2606, -0.1198],\n",
      "        [-0.0052,  0.2361, -0.2004,  ...,  0.3149,  0.3627,  0.0131],\n",
      "        [ 0.0593,  0.1619, -0.1682,  ...,  0.3164,  0.3835,  0.0134]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.1898, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([256, 65])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Now we will add the self attention head to this bigram model\n",
    "# Constructing the bigram model\n",
    "## In the Bigram model it does not really matter the position since you will only be contructing or predicting based on the one character before\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.token_position_embedding_table = nn.Embedding(block_size, n_emb) # position embedding of one batch in the context\n",
    "        self.sa_heads = MultiHeadAttention(4, n_emb // 4) ## i.e. 4 head for 8 dimension i.e. 32 / 4 = 8\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        B,T = idx.shape # B-Batch, T-Tokens, C-Channel i.e. dimensions\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C) (32,8,32)\n",
    "        position_emb = self.token_position_embedding_table(torch.arange(T)) ## for each block is arrange as 0,1,2,3\n",
    "        x = token_emb + position_emb\n",
    "        x = self.sa_heads(x) \n",
    "        logits = self.lm_head(x) # xW.transpose() + b \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # cropping the idx so that in the position embedding it is not bigger than the expecting so we will crop it\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_crop) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 32 # this is basically the dimensions size\n",
    "batch_size = 32\n",
    "head_size = 32\n",
    "block_size = 8\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "print(targets.shape) # (B*T, C)\n",
    "# # torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# # This is completely random generation as the model is untrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3804d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.189296245574951\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "## Training a model\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2ba5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hadd wist. Be siths\n",
      "you mman in epare, aris? merow\n",
      "Ther let his? wpit bar, arearind, cize wit and;\n",
      "Gelaser' noth\n",
      "Me\n",
      "By nothive nich out hat belt anp'y st bazel\n",
      "Teetiser, de'll thon theearmighto ford woudis selisen, his my de'llowne's,\n",
      "I bects, wis bee! ase may sin coof thee 'dipd\n",
      "Aw hathonger vit ouldy:\n",
      "Off bust doust,\n",
      "Gugh obleeakeall are frrabe atne pire not\n",
      "Yand ay out sith uplest have bell brorfordy bold.\n",
      "MIG EDWIHARTeldeatis the in afire, payst shat bleay gis selwerlish Orent meevereds.\n",
      "\n",
      "Ther dunrd mond youch fas prise wan,\n",
      "Tore bak onessps my have dis loTu though math hain pil to uth,\n",
      "Tand sleser,\n",
      "Mwobor with nothy them, of mar cof. Handme toousem hiold not os bund.\n",
      "\n",
      "Porempstay no's andt:\n",
      "Sw ann of all of lights mend movis st?\n",
      "\n",
      "ARWIR:\n",
      "Alhe pas ime wis the tohe ord ofet clin; heme for soo,\n",
      "Wish nor the you deacegir of my ame in no--w--wingivert, for ovingien be.\n",
      "\n",
      "Dof wat prot prrow fram you the hen, halike ture wengh ther, to poy a forn theace:\n",
      "Thand cfimient.\n",
      "\n",
      "I vecarl,\n",
      "Apill id,\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c82e9",
   "metadata": {},
   "source": [
    "### Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a0b3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, n_emb),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae02c964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 32)\n",
      "  (token_position_embedding_table): Embedding(8, 32)\n",
      "  (sa_heads): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
      "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fwd): FeedForwardLayer(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n",
      "logits: tensor([[-0.0157, -0.0976,  0.0721,  ..., -0.0107,  0.0436,  0.0568],\n",
      "        [-0.0756, -0.0747,  0.1014,  ..., -0.0352, -0.0082,  0.0192],\n",
      "        [-0.0959, -0.1556, -0.0010,  ..., -0.1381,  0.0197,  0.0535],\n",
      "        ...,\n",
      "        [-0.0951,  0.0787,  0.1760,  ..., -0.0212,  0.0320,  0.0368],\n",
      "        [-0.0633,  0.0604,  0.2020,  ...,  0.0719, -0.0374, -0.0317],\n",
      "        [-0.0654,  0.0168,  0.2080,  ...,  0.0636, -0.0416, -0.0417]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.1946, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([256, 65])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Now we will add the self attention head to this bigram model\n",
    "# Constructing the bigram model\n",
    "## In the Bigram model it does not really matter the position since you will only be contructing or predicting based on the one character before\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.token_position_embedding_table = nn.Embedding(block_size, n_emb) # position embedding of one batch in the context\n",
    "        self.sa_heads = MultiHeadAttention(4, n_emb // 4) ## i.e. 4 head for 8 dimension i.e. 32 / 4 = 8\n",
    "        self.fwd = FeedForwardLayer(n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        B,T = idx.shape # B-Batch, T-Tokens, C-Channel i.e. dimensions\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C) (32,8,32)\n",
    "        position_emb = self.token_position_embedding_table(torch.arange(T)) ## for each block is arrange as 0,1,2,3\n",
    "        x = token_emb + position_emb\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.fwd(x)\n",
    "        logits = self.lm_head(x) # xW.transpose() + b \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # cropping the idx so that in the position embedding it is not bigger than the expecting so we will crop it\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_crop) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 32 # this is basically the dimensions size\n",
    "batch_size = 32\n",
    "head_size = 32\n",
    "block_size = 8\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "print(targets.shape) # (B*T, C)\n",
    "# # torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# # This is completely random generation as the model is untrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "782e0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.149420738220215\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "## Training a model\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26dedce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Surlet ofsce'im nal-mapp;\n",
      "Dupon now\n",
      "The foall ich\n",
      "ARICERSTERILEORICUC:\n",
      "Mallingear:\n",
      "His; bulist ther ins fran bre nanis therequoutt,\n",
      "Fortonce sre, his of tin.\n",
      "Mome,\n",
      "Lorre prurnust nough by fnot yout not wont\n",
      "LOUWeare.\n",
      "\n",
      "CONIUS:\n",
      "Cauld witan persbous\n",
      "My met.\n",
      "\n",
      "Dombare batoor!\n",
      "\n",
      "PUESTONT:\n",
      "Govirend\n",
      "How of id ithoust poltbe sinoth fir,\n",
      "Thyatizeserm.\n",
      "\n",
      "Put it gre, glat?\n",
      "An bithis hown at this! shionelr a for buthates.\n",
      "\n",
      "KINGLERIRCLOT:\n",
      "Noliel;\n",
      "At ifeettoot held.\n",
      "\n",
      "Clakerecest.\n",
      "\n",
      "Sutherd, bith llimthe irs beaind lan agood for thoulke you.\n",
      "Shid pus Lorints;\n",
      "That beast sard.\n",
      "\n",
      "RIORGLUS:\n",
      "Mith Walind cures jlory theat and\n",
      "COPERESTIY:\n",
      "Thiusttorplestroone he the?\n",
      "Coo poong ting he sipnoardlalist it yout joothavhllwerd maces:\n",
      "I\n",
      "Het\n",
      "Loropeyser, milessole's the pist bont ot, im and? coled: therakerse, takee to les-tea ited adifulth them to of ald so the thim nebe sere,\n",
      "Hepont!\n",
      "\n",
      "Vin it.\n",
      "Io have paidgafitery han, a man, whand you.\n",
      "\n",
      "KINGHONY:\n",
      "That, ther.\n",
      "\n",
      "FIO:\n",
      "Toweeou fais is suth in to the curtt ensssat truled th\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea788aa",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "16639a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a head module\n",
    "# attention = softmax((q @ wT)/d**0.5)) @ v\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        self.key = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        self.value = nn.Linear(in_features = n_emb, out_features = head_size, bias = False)\n",
    "        # here tril is not a paramter it is a buffer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        # self.head_size = head_size\n",
    "        self.dropout = nn.Dropout(dropout) # works as a regularlization where number of node from the layer has been dropout\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # B,T,C\n",
    "        q = self.query(x) # B,H,T\n",
    "        k = self.key(x) # B,H,T\n",
    "        # compute attention score\n",
    "        # ((q @ wT)/d**0.5))\n",
    "        # wei = q @ k.transpose(-2,-1) * self.head_size**-0.5 # B,T,H @ B,H,T --> B,T,T\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # B,T,H @ B,H,T --> B,T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1) # softmax((q @ wT)/d**0.5))\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) \n",
    "        out = wei @ v # softmax((q @ wT)/d**0.5)) @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bfa22ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "         out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "         out = self.proj(out)\n",
    "         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4692af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb), ### This will increase the number of nodes when moving to the non-linear layer i.e. ReLU\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_emb, n_emb), ### Here we come back to the same dimension\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ebcf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_emb // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) ## i.e. 4 head for 8 dimension i.e. 32 / 4 = 8\n",
    "        self.fwd = FeedForwardLayer(n_emb)\n",
    "        self.ln1 = nn.LayerNorm(n_emb) #### we will be adding the linear normalization also so it can make in gaussian distribution\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x)) ### adding the skip connection\n",
    "        x = x + self.fwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14a7074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 384)\n",
      "  (token_position_embedding_table): Embedding(256, 384)\n",
      "  (block): Sequential(\n",
      "    (0): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa_heads): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x Head(\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (fwd): FeedForwardLayer(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n",
      "cuda\n",
      "Model running on: cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "logits: tensor([[ 0.1328,  0.2608,  0.4102,  ...,  0.2735,  0.1710,  1.0562],\n",
      "        [ 0.3615, -0.1402,  0.7497,  ...,  0.0348, -0.6489,  0.5960],\n",
      "        [-0.8929,  0.5919,  0.9456,  ..., -0.1354, -0.5897, -0.0096],\n",
      "        ...,\n",
      "        [-0.4959,  0.2332,  0.5140,  ..., -0.1198, -0.0590,  0.6135],\n",
      "        [ 0.4179,  0.6206,  0.1170,  ...,  0.2444, -0.0848, -0.1820],\n",
      "        [-0.6858, -0.1394,  0.1438,  ..., -0.1632, -0.5355,  0.6125]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "loss: tensor(4.2875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([16384, 65])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Now we will add the blocks to this bigram model\n",
    "# Constructing the bigram model\n",
    "## In the Bigram model it does not really matter the position since you will only be contructing or predicting based on the one character before\n",
    "## This is decoder only transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb) # this is random initialization which will be learned during training.\n",
    "        self.token_position_embedding_table = nn.Embedding(block_size, n_emb) # position embedding of one batch in the context\n",
    "        # self.block = nn.Sequential(\n",
    "        #     Block(n_emb, n_head),\n",
    "        #     Block(n_emb, n_head),\n",
    "        #     Block(n_emb, n_head),\n",
    "        #     nn.LayerNorm(n_emb),\n",
    "        # )\n",
    "        # Cosmetic changes\n",
    "        self.block = nn.Sequential(*[Block(n_emb, n_head) for _ in range(n_layer)])\n",
    "        self.ln_norm = nn.LayerNorm(n_emb) # layer_normalization\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers where idx is xb and targets is yb\n",
    "        # idx = idx.to(self.embedding.weight.device)\n",
    "        B,T = idx.shape # B-Batch, T-Tokens, C-Channel i.e. dimensions\n",
    "        # print(\"idx:\", idx.device, \" embedding:\", self.embedding.weight.device)\n",
    "        token_emb = self.token_embedding_table(idx)  # (B,T,C) (32,8,32)\n",
    "        position_emb = self.token_position_embedding_table(torch.arange(T, device=idx.device)) ## for each block is arrange as 0,1,2,3\n",
    "        x = token_emb + position_emb\n",
    "        x = self.block(x)\n",
    "        x = self.ln_norm(x)\n",
    "        logits = self.lm_head(x) # xW.transpose() + b \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # Throw-away variable in loops so it does not fill the variable memory.\n",
    "            # cropping the idx so that in the position embedding it is not bigger than the expecting so we will crop it\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_crop) # Calls the Forward method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1) #  draws a sample from the probability distribution. It returns the index of the next chosen token.\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "n_emb = 384 # this is basically the dimensions size\n",
    "batch_size = 64 # number of batches in one go\n",
    "head_size = 32\n",
    "max_iter = 5000\n",
    "block_size = 256 ### context length\n",
    "learning_rate = 3e-04\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "m = BigramLanguageModel()\n",
    "print(m)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "m = m.to(device)\n",
    "print(device)\n",
    "print(\"Model running on:\", next(m.parameters()).device)\n",
    "xb = xb.to(device)\n",
    "print(xb.device)\n",
    "print(yb.device)\n",
    "yb = yb.to(device)\n",
    "logits, loss = m(xb, yb) # PyTorch overloads the __call__() method of nn.Module. This calls the forward() method. \n",
    "print(\"logits:\", logits)\n",
    "print(\"loss:\", loss)\n",
    "print(logits.shape) # (B*T, C)\n",
    "print(targets.shape) # (B*T, C)\n",
    "\n",
    "# # torch.zeroes is for new line character so we can initialize the generation from it.\n",
    "# # This is completely random generation as the model is untrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e2b68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model running on: cuda:0\n",
      "1.1135485172271729\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "m = m.to(device)\n",
    "print(\"Model running on:\", next(m.parameters()).device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "## Training a model\n",
    "for steps in range(max_iter):\n",
    "    xb,yb = get_batch('train')\n",
    "    xb = xb.to(device)   # move training batch to GPU\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da499120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hands your form, she was married\n",
      "'Twixt sorrow; and here was never diverse;\n",
      "Linely in borderous may party of this,\n",
      "That, that dasband swallow'd me furthers to do,\n",
      "Hold rest not thy country's bless, than grossess'd,\n",
      "What mutinous is the palment discipulate?\n",
      "Methinks I am too began?\n",
      "\n",
      "FRIAR\n",
      "MIANCE:\n",
      "OR Isabel counsent to a truth of mind mockery.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Mine army right, I am a poor true-violaty.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I cannot keep and deservice;\n",
      "But seem I mach the subdue that majesty.\n",
      "\n",
      "KING RICHARD II:\n",
      "Exeor, thou cansures' things hath done that;\n",
      "For break of death, blest whilst I say thee,\n",
      "Thou art of a bolts, not bad and the subject.\n",
      "Though artiest their noses in heaven closed\n",
      "With thembles thy held too noble and will not\n",
      "Then a husband's claim tears, and manifesty\n",
      "To say the base and the son of dulgrees\n",
      "Ere thou comfort the former croosile that answer\n",
      "Shall begin a blemish shame to hide to besome\n",
      "That lock the and horses serly woman's course.\n",
      "\n",
      "ARCHBISHOP OF YORK:\n",
      "What! O chaffedil vance! thou think'st to flow.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "You sport!\n",
      "\n",
      "ISABELLA:\n",
      "O me this word and gally met,\n",
      "And what's you detest the fell of the people,\n",
      "And will ay from them, as hourly dreams.\n",
      "\n",
      "GLOUCESTER:\n",
      "'Tis hard that any thing, my lord's son;\n",
      "And I hope to answer the Antiate-Claudio.\n",
      "\n",
      "ANGELO:\n",
      "Gracious me I am longther quarrely; good judge the king,\n",
      "But marques for tumbler, falsehoody and at\n",
      "About the morning: here's a topta\n",
      "Or-master, the tongue's shed; where the lord a\n",
      "cushions of Duke of Gloucester-oved; brother ording\n",
      "And unto the season of bluckets!\n",
      "\n",
      "ORCAPULET:\n",
      "A setseemen'd heads prizently nobly;\n",
      "And, as every sone achied to begar that,\n",
      "As for every free our soldiers,\n",
      "And after us we seem, our majesty\n",
      "As detesty and the fee?\n",
      "O Thus salt and dream tortures;\n",
      "And woe's to body'st is your grace.\n",
      "\n",
      "MOPSA:\n",
      "My devortion\n",
      "Will to your son out of foul like or top?\n",
      "\n",
      "AUTOLYCUS:\n",
      "Ay', mine; ahappell'd indeed!\n",
      "Are that thou dost rain!\n",
      "\n",
      "LORD OF CAMILLO:\n",
      "I dare the tout this hour rogue?' at they\n",
      "tribunes will, were two devil, and the proof woes\n",
      "A traitor deed as in grawth. O preseration,\n",
      "Thou art fitted to Martina's death, and bear them?\n",
      "They see that womb or headst them so their officer\n",
      "As Richard their late towards are salt to image\n",
      "The shees and two appoint: between under theirs,\n",
      "Yet one the safe, and I am sold so absence:\n",
      "Yea, already Buck to the story's time hears;\n",
      "Into a young 'factious flower,\n",
      "Cleed out off, I'll dismist them that glorious.\n",
      "\n",
      "SICINIUS:\n",
      "Leave her to shame: when you shall bear them to in\n",
      "Gipsting to the dribs, and trespections; which he\n",
      "looking: the prayer I am gled charged myself.\n",
      "\n",
      "BRUTUS:\n",
      "Seize what a son I am?\n",
      "\n",
      "SICINIUS:\n",
      "Unevenly kiss not held: the meanner I lay him in\n",
      "her: I say, since the measure they ear in one so piece.\n",
      "\n",
      "BRUTUS:\n",
      "You are told to the sileness; yet them what I know him\n",
      "I appoint of so pere disgrace.\n",
      "\n",
      "SICINIUS:\n",
      "Their able tendering! here's meeting.\n",
      "Three you rest defiser to the deadly,\n",
      "Can contempt them all dost their steether scord.\n",
      "\n",
      "SUCINIUS:\n",
      "To speak theirs?\n",
      "\n",
      "Mest Consordian:\n",
      "So; therefore, I know them alone at thing.\n",
      "\n",
      "BRUTUS:\n",
      "Call my lord and cheerable close\n",
      "toments; but rash by false-case of valour,\n",
      "lected me with live: you have been else, though'd must\n",
      "too leaning thee seat, and angel ingratity\n",
      "with a great from thing! you under ground? Profords these offer\n",
      "alsoemies and hear, I chate your devoution?\n",
      "\n",
      "CAMILLO:\n",
      "I pray you, lord; you are all apparel but your old,\n",
      "to knowly no slow croling but away with yourselves;\n",
      "power your name is as the public would\n",
      "upon the adulous but allence; for I will not\n",
      "lipt to be askard ministeries.\n",
      "\n",
      "Shepherd:\n",
      "O Capulets!\n",
      "\n",
      "Are here, sir; is't, they considered: stay call'd\n",
      "prove of this; for a festimong wicked goass, grant us,\n",
      "and most false.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "My master judgment at my bird a bond;\n",
      "And you have made ink with dances\n",
      "Of me aughtifers mortal these one wild be so,\n",
      "Than I can dreadful or from my father.\n",
      "\n",
      "CARISBUS:\n",
      "Whence's there?\n",
      "\n",
      "HASTINGS:\n",
      "Years are the Lady Stan's first;\n",
      "Seeing the Froth Gians and Sir Stonh.\n",
      "\n",
      "STANLEY:\n",
      "Asseng is Lancaster, York your order conditions.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "No gods, but not knowsledge, fetch when Wales\n",
      "Will his manners in his ement.\n",
      "\n",
      "CLIFFORD:\n",
      "No, welcome, forship are even of the table;\n",
      "The hope's in his while heart out they but his;\n",
      "His life to gross, he foundless that kill'd,\n",
      "Gorged high sway'd, and awhore, sweet his violences!\n",
      "The noble Edward leave and clamour'd,\n",
      "Came shrip'd to me of himself door hate withdrawth.\n",
      "\n",
      "GREEN:\n",
      "Need, 'tin bands he do;'\n",
      "And, Grey, the tide doth rage of our blocks:\n",
      "So dead, my lord, let Essbury.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Evex new my master Warwick to in judgmant cheer,\n",
      "And, infortune the insterness, goast may be long.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sweet Salbors and QuencaCH.\n",
      "\n",
      "LADY OVERBY:\n",
      "Well, you you have leng have this pratties;\n",
      "And now, that respecting of Bushy, God, we\n",
      "Were convided some that he knew our air,\n",
      "I am alone: but, wilt unsointed?\n",
      "It was the news, sir, the gift of another;\n",
      "That bears, down: and all's a' men,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long, device = device), max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fd7039d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/projects/.immune/Personal/Transformers_Tutorial'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e579c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only model weights are saved\n",
    "torch.save(m.state_dict(), \"bigram_model.pt\")\n",
    "# later to load the model\n",
    "# m = BigramLanguageModel()            # recreate the model architecture\n",
    "# m.load_state_dict(torch.load(\"bigram_model.pt\"))\n",
    "# m.to(device)                         # move to GPU if needed\n",
    "# m.eval()             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226470b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9b113fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(m, \"bigram_model_full.pt\")\n",
    "# later to load the model\n",
    "# m = BigramLanguageModel()            # recreate the model architecture\n",
    "# m.load_state_dict(torch.load(\"bigram_model.pt\"))\n",
    "# m.to(device)                         # move to GPU if needed\n",
    "# m.eval()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "413ddf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/projects/.immune/Personal/Transformers_Tutorial'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03b918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacec_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
