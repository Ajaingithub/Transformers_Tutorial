{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3744cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# conda activate torch_gpu\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ccbf20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is combination of 2 classes we generated in gpt_dev_aj i.e. class Head and MultiHeadAttention\n",
    "class CausalSelfAttention(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need to approximate it but we are replicating GPT2 so we will keep this\n",
    "# GELU is similar to RELU instead it does not have any dead neuron as in RELU if x < 0 so y = 0\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(approximate = \"tanh\"),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.Dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f207dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # head_size = self.n_embd // self.n_head\n",
    "        # self.sa_heads = MultiHeadAttention(self.n_head, head_size)\n",
    "        # self.fwd = FeedForwardLayer(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257\n",
    "    block_size: int = 1024\n",
    "    n_embd: int = 768\n",
    "    dropout: int = 0.1\n",
    "    lr: int = 3e-04\n",
    "    nlayer = 12\n",
    "\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    self.transformer = nn.ModuleDict(Dict{\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        h = nn.Sequential(*[Block(config) for _ in config.nlayer])\n",
    "        self.ln_norm = nn.LayerNorm(config.n_embd)\n",
    "    })\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed16e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
